% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Introduction of package ClusterR},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Introduction of package ClusterR}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\# enable table of contents \# enable numbering sections \#toc: true
\#number\_sections: true

\#Introduction

ClusterR, as the name says, is an R package with some tools for
clustering data.

Clustering data is a technique used in the unsupervised learning
framework, where for each observation \(1 \leq i \leq n\), is given only
the vector \(x_i\) of the measurements, without any associated response
variable \(y_i\).

So, since we do not have \(y\) we cannot apply any type of checks on the
error between real and predicted values for the response variable.

Documentation can be found at
\url{https://cran.r-project.org/web/packages/ClusterR/}.

\#Summary

---\textgreater{} dataset sintetici 2d per visualizzazioni punti
generati da gaussiane punti generati da PCA di un dataset grande
-\textgreater{} proteine?

\begin{itemize}
\item
  Introduzione al clustering (cos'Ã¨, immagini d'esempio, esempi di
  applicazione)
\item
  Descrizione dei tipi di clustering
\item
\begin{verbatim}
  Connectivity based
\end{verbatim}
\item
\begin{verbatim}
  Centroid based
\end{verbatim}
\item
\begin{verbatim}
  Distribution based
\end{verbatim}
\item
\begin{verbatim}
  Density based
\end{verbatim}
\item
  Dataset di esempio della libreria
\item
  Clustering supportati da ClusterR (centroid based, distribution based)
\item
  Centroid based clustering (k-means, mini-batch-kmeans, k-medoids)
\item
\begin{verbatim}
  k-means basic usage - KMeans_rcpp
\end{verbatim}
\item
\begin{verbatim}
  variare il numero di k - immagini per diversi k
\end{verbatim}
\item
\begin{verbatim}
  optimal k - Optimal_Clusters_KMeans 
\end{verbatim}
\item
\begin{verbatim}
  visualizzare la bonta del clustering - Silouhette plot
\end{verbatim}
\item
\begin{verbatim}
  misura del tempo di esecuzione - large dataset - KMeans_rcpp vs KMeans_arma vs mini-batck KMeans
\end{verbatim}
\item
\begin{verbatim}
  medoids vs centroids - Cluster_Medoids
\end{verbatim}
\item
\begin{verbatim}
  misura del tempo di esecuzione - Cluster_Medoids vs Clara_Medoids
\end{verbatim}
\item
  Distribution based clustering
\item
\begin{verbatim}
  Gaussian Mixture Models Optimal_Clusters_GMM 
\end{verbatim}
\item
\begin{verbatim}
  visualizzazione 2d gaussiane
\end{verbatim}
\item
  Distribution vs Centroid
\item
\begin{verbatim}
  differenze clustering usando i dataset sintetici
\end{verbatim}
\item
\begin{verbatim}
  external_validation per validazione con true label 
\end{verbatim}
\item
  (optional) paragone ClusteR vs ClusterR
\end{itemize}

Clustering is the task of dividing the population or data points into a
number of groups such that data points in the same groups are more
similar to other data points in the same group and dissimilar to the
data points in other groups. It is basically a collection of objects on
the basis of similarity and dissimilarity between them.

Clustering is one of the unsupervised learning methods: it means that we
have no references or labeled responses. For this reason the clustering
algorithms can only rely on the given data, regardless of the specific
task we are trying to solve. Clustering is suitable for explaining the
possible data-generating process, for finding possible relations or
hierarchies between data or as a part of the exploratory data analysis.

For doing clustering is necessary to define a measure of similarity:
similar items should belong to the same group. A measure of similarity
must be non-negative, symmetric and must have a value of zero only if
the items are equal. There are various possible similarity measures, for
example: euclidean distance, manhattan distance and maximum distance. A
clustering library should give the possibility to specify the similarity
function to be used.

A predictive algorithm based on clustering may for example use the value
of nearest cluster to give the response for a new data point. It is also
possible to combine the informations of all clusters, weighted by the
cluster distances.

There are four different types of clustering: - Connectivity based -
Centroid based - Distribution based - Density based

Connectivity-based clustering tries to create connections between the
clusters. In this way the algorithm creates a tree (or a forest), which
can give us an hierarchial view of the data. Two possible uses can be
creating a cause-effect relationship between items or reconstructing a
possible evolution of a population.

Centroid-based clustering is based on centroids: each item belongs to
the class of its nearest centroid. The centroids can be restricted to
locations of the items (medoid) or generic points (centroid). The number
of centroids can be hard-coded or searched by the algorithm; the
position of the centroids is optimized by the algorithm. In general it
is better to have the minimum number of centroids that explains well the
data (simplest model).

Distribution-based clustering uses a probabilistic model to explain the
original data-generating model, by minimizing the discrepancy. By doing
so, it is possible to create specific models that accurately describe
the data and give an idea of the prediction confidence. A general way to
describe data is by using Gaussian Mixture Models.

Density-based clustering groups data using distance between points.
Dense connected regions of points should be part of the same cluster. In
this way there is not a fixed number of clusters, but the number of
clusters depends on how many different regions of data points are there
and on the chosen density threshold. Two examples of density-based
clustering algorithms are DBSCAN and OPTICS.

ClusterR is a package designed for cluster generation and analysis. It
combines different clustering algorithms, along with helper functions
and example datasets.

The package is subdivided in different parts: - clustering algorithms -
searching for optimal number of clusters of the clustering algorithms -
prediction using the generated clusters - visualization of the
clustering goodness using 2d scatterplots and silouhette dissimilarity
plots - validation of the clustering goodness using ground truth labels
- helper methods

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{center\_scale}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, mean_center = TRUE, sd_scale = TRUE) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!is.logical(mean_center)) 
##         stop("the mean_center parameter should be either TRUE or FALSE")
##     if (!is.logical(sd_scale)) 
##         stop("the sd_scale parameter should be either TRUE or FALSE")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = SCALE(data, mean_center, sd_scale)
##     return(res)
## }
## <bytecode: 0x000001c58ba1cae8>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{plot\_2d}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, clusters, centroids_medoids) 
## {
##     if (grDevices::dev.cur() != 1) {
##         grDevices::dev.off()
##     }
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if ("data.frame" %in% class(centroids_medoids)) 
##         centroids_medoids = as.matrix(centroids_medoids)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (is.integer(clusters)) 
##         clusters = as.numeric(clusters)
##     if (!is.vector(clusters) || !inherits(clusters, "numeric")) 
##         stop("The \"clusters\" parameter has to be a numeric vector!")
##     if (!inherits(centroids_medoids, "matrix") || nrow(centroids_medoids) != 
##         length(unique(clusters)) || ncol(centroids_medoids) != 
##         ncol(data)) 
##         stop("centroids_medoids should be a matrix with number of rows equal to the unique labels of clusters and number of columns equal to the number of columns of the data")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     if (length(unique(as.vector(clusters))) > 26) 
##         stop("valid shape values are from 0 to 25, consider to reduce the number of class-levels")
##     if (ncol(data) != 2) 
##         stop("the data should be 2-dimensional")
##     levs = as.factor(paste0("cluster ", 1:length(unique(as.vector(clusters)))))[match(as.vector(clusters), 
##         sort(unique(as.vector(clusters))))]
##     df_plot = data.frame(data, clusters = levs)
##     colnames(df_plot) = c("x", "y", "clusters")
##     add_points = data.frame(centroids_medoids)
##     add_points$clusters = as.factor(paste0("centroid or medoid ", 
##         1:nrow(add_points)))
##     colnames(add_points) = c("x", "y", "clusters")
##     ggplot2::ggplot(df_plot, ggplot2::aes(x = x, y = y, group = clusters)) + 
##         ggplot2::geom_point(ggplot2::aes(shape = clusters, color = clusters), 
##             size = 2.5) + ggplot2::geom_point(data = add_points, 
##         ggplot2::aes(x = x, y = y, shape = clusters, color = clusters), 
##         size = 5) + ggplot2::scale_shape_manual(values = 0:(length(unique(as.vector(clusters))) * 
##         2)) + ggplot2::scale_size_manual(values = c(5, 3, 4)) + 
##         ggplot2::theme(legend.position = "right") + ggplot2::theme(legend.title = ggplot2::element_blank())
## }
## <bytecode: 0x000001c58bb25ae0>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{AP\_affinity\_propagation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, p, maxits = 1000, convits = 100, dampfact = 0.9, 
##     details = FALSE, nonoise = 0, time = FALSE) 
## {
##     if (!inherits(data, "matrix")) 
##         stop("The 'data' parameter should be a matrix!", call. = F)
##     lst_res = affinity_propagation(data, p, maxits, convits, 
##         dampfact, details, nonoise, 2.2204e-16, time)
##     vec_clust = rep(NA, nrow(data))
##     ap_clust = lst_res$clusters
##     nams_ap_clust = names(ap_clust)
##     for (x in 1:length(ap_clust)) {
##         vec_clust[ap_clust[[x]] + 1] = as.integer(nams_ap_clust[x])
##     }
##     lst_res[["clusters_vectorized"]] = vec_clust
##     return(lst_res)
## }
## <bytecode: 0x000001c58bb99b10>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{AP\_preferenceRange}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, method = "bound", threads = 1) 
## {
##     if (!inherits(data, "matrix")) 
##         stop("The 'data' parameter should be a matrix!", call. = F)
##     if (!method %in% c("bound", "exact")) 
##         stop("The 'method' parameter should be either 'bound' or 'exact'!", 
##             call. = F)
##     return(preferenceRange(data, method, threads))
## }
## <bytecode: 0x000001c58bc15550>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{cost\_clusters\_from\_dissim\_medoids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, medoids) 
## {
##     if (nrow(data) != ncol(data)) 
##         stop("I expect that the input data object is a dissimilarity matrix where the number of rows equals the number of columns!")
##     if (!inherits(medoids, c("numeric", "integer"))) 
##         stop("I expect the 'medoids' input object to be a numeric vector!")
##     res_clust = cost_clusters_from_dis_meds(dissim_mat = data, 
##         medoids = medoids - 1)
##     return(list(cost = res_clust$cost, clusters = as.vector(res_clust$clusters)))
## }
## <bytecode: 0x000001c58bd16b10>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{distance\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, method = "euclidean", upper = FALSE, diagonal = FALSE, 
##     minkowski_p = 1, threads = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!method %in% c("euclidean", "manhattan", "chebyshev", 
##         "canberra", "braycurtis", "pearson_correlation", "simple_matching_coefficient", 
##         "minkowski", "hamming", "jaccard_coefficient", "Rao_coefficient", 
##         "mahalanobis", "cosine")) 
##         stop("the method should be one of 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient',\n         'minkowski', 'hamming', 'jaccard_coefficient', 'Rao_coefficient', 'mahalanobis', 'cosine'")
##     if (!is.logical(upper)) 
##         stop("the upper parameter should be either TRUE or FALSE")
##     if (!is.logical(diagonal)) 
##         stop("the diagonal parameter should be either TRUE or FALSE")
##     if (method == "minkowski" && minkowski_p == 0) 
##         stop("if distance metric is minkowski then the minkowski_p should be either a positive or a negative number but not 0.0")
##     if (threads < 1) 
##         stop("the number of threads should be greater than 1")
##     res = dissim_mat(data, method, minkowski_p, upper, diagonal, 
##         threads, 1e-06)
##     return(res)
## }
## <bytecode: 0x000001c58bd94fb0>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{external\_validation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (true_labels, clusters, method = "adjusted_rand_index", 
##     summary_stats = FALSE) 
## {
##     if (is.integer(true_labels)) 
##         true_labels = as.numeric(true_labels)
##     if (is.integer(clusters)) 
##         clusters = as.numeric(clusters)
##     if (!is.vector(true_labels) || !is.numeric(true_labels)) 
##         stop("true_labels should be a numeric vector")
##     if (!is.vector(clusters) || !is.numeric(clusters)) 
##         stop("clusters should be a numeric vector")
##     if (length(true_labels) != length(clusters)) 
##         stop("the length of the true_labels vector should equal the length of the clusters vector")
##     if (!method %in% c("rand_index", "adjusted_rand_index", "jaccard_index", 
##         "fowlkes_mallows_index", "mirkin_metric", "purity", "entropy", 
##         "nmi", "var_info", "nvi")) 
##         stop("supported methods are 'rand_index', 'adjusted_rand_index', 'jaccard_index', 'fowlkes_mallows_index', 'mirkin_metric', 'purity', 'entropy', 'nmi', 'var_info', 'nvi'")
##     tbl = table(clusters, true_labels)
##     conv_df = as.data.frame.matrix(tbl)
##     tp_plus_fp = sum(gmp::asNumeric(gmp::chooseZ(rowSums(conv_df), 
##         2)))
##     tp_plus_fn = sum(gmp::asNumeric(gmp::chooseZ(colSums(conv_df), 
##         2)))
##     tp = sum(gmp::asNumeric(gmp::chooseZ(as.vector(as.matrix(conv_df)), 
##         2)))
##     fp = tp_plus_fp - tp
##     fn = tp_plus_fn - tp
##     tn = gmp::asNumeric(gmp::chooseZ(sum(as.vector(as.matrix(conv_df))), 
##         2)) - tp - fp - fn
##     if (summary_stats || method == "adjusted_rand_index") {
##         prod_comb = (tp_plus_fp * tp_plus_fn)/gmp::asNumeric(gmp::chooseZ(length(true_labels), 
##             2))
##         mean_comb = (tp_plus_fp + tp_plus_fn)/2
##     }
##     if (summary_stats || method == "purity") {
##         tmp_pur = apply(conv_df, 1, max)
##         res_purity = sum(tmp_pur)/length(true_labels)
##     }
##     if (summary_stats || method == "entropy") {
##         tmp_entropy = sum(apply(conv_df, 2, function(x) entropy_formula(x)))
##         res_entropy = -(1/(sum(tbl) * log2(length(unique(true_labels))))) * 
##             tmp_entropy
##     }
##     if (summary_stats || method == "nmi" || method == "var_info" || 
##         method == "nvi") {
##         mutual_information = 0
##         joint_entropy = 0
##         for (i in 1:nrow(conv_df)) {
##             for (j in 1:ncol(conv_df)) {
##                 if (conv_df[i, j] > 0) {
##                   joint_entropy = joint_entropy + (-((conv_df[i, 
##                     j]/sum(tbl)) * log2(conv_df[i, j]/sum(tbl))))
##                   mutual_information = mutual_information + ((conv_df[i, 
##                     j]/sum(tbl)) * log2(as.numeric(gmp::as.bigz(as.numeric(sum(tbl)) * 
##                     as.numeric(conv_df[i, j]))/gmp::as.bigz(as.numeric(sum(conv_df[i, 
##                     ])) * as.numeric(sum(conv_df[, j]))))))
##                 }
##             }
##         }
##         entr_cluster = sum(apply(conv_df, 1, function(x) -(sum(x)/sum(tbl)) * 
##             log2(sum(x)/sum(tbl))))
##         entr_class = sum(apply(conv_df, 2, function(x) -(sum(x)/sum(tbl)) * 
##             log2(sum(x)/sum(tbl))))
##         if (summary_stats || method == "nmi" || method == "nvi") {
##             unq_true = unique(true_labels)
##             unq_clust = unique(clusters)
##             if (length(unq_true) == 1 && length(unq_clust) == 
##                 1) {
##                 NMI = 1
##                 NVI = 1
##             }
##             else {
##                 NMI = (mutual_information/((entr_cluster + entr_class)/2))
##                 NVI = 1 - (mutual_information/joint_entropy)
##             }
##         }
##         VAR_INFO = (entr_cluster + entr_class) - 2 * mutual_information
##     }
##     if (summary_stats) {
##         prec = tp/(tp + fp)
##         rec = tp/(tp + fn)
##         cat("", "\n")
##         cat("----------------------------------------", "\n")
##         cat("purity                         :", round(res_purity, 
##             4), "\n")
##         cat("entropy                        :", round(res_entropy, 
##             4), "\n")
##         cat("normalized mutual information  :", round(NMI, 4), 
##             "\n")
##         cat("variation of information       :", round(VAR_INFO, 
##             4), "\n")
##         cat("normalized var. of information :", round(NVI, 4), 
##             "\n")
##         cat("----------------------------------------", "\n")
##         cat("specificity                    :", round(tn/(tn + 
##             fp), 4), "\n")
##         cat("sensitivity                    :", round(tp/(tp + 
##             fn), 4), "\n")
##         cat("precision                      :", round(prec, 4), 
##             "\n")
##         cat("recall                         :", round(rec, 4), 
##             "\n")
##         cat("F-measure                      :", round(2 * ((prec * 
##             rec)/(prec + rec)), 4), "\n")
##         cat("----------------------------------------", "\n")
##         cat("accuracy OR rand-index         :", round((tp + tn)/(tp + 
##             fp + fn + tn), 4), "\n")
##         cat("adjusted-rand-index            :", round((tp - prod_comb)/(mean_comb - 
##             prod_comb), 4), "\n")
##         cat("jaccard-index                  :", round(tp/(tp + 
##             fp + fn), 4), "\n")
##         cat("fowlkes-mallows-index          :", round(sqrt((tp/((tp + 
##             fp))) * (tp/(tp + fn))), 4), "\n")
##         cat("mirkin-metric                  :", round(2 * (fp + 
##             fn), 4), "\n")
##         cat("----------------------------------------", "\n")
##     }
##     if (method == "rand_index") {
##         return((tp + tn)/(tp + fp + fn + tn))
##     }
##     if (method == "adjusted_rand_index") {
##         return((tp - prod_comb)/(mean_comb - prod_comb))
##     }
##     if (method == "jaccard_index") {
##         return(tp/(tp + fp + fn))
##     }
##     if (method == "fowlkes_mallows_index") {
##         return(sqrt((tp/((tp + fp))) * (tp/(tp + fn))))
##     }
##     if (method == "mirkin_metric") {
##         return(2 * (fp + fn))
##     }
##     if (method == "purity") {
##         return(res_purity)
##     }
##     if (method == "entropy") {
##         return(res_entropy)
##     }
##     if (method == "nmi") {
##         return(NMI)
##     }
##     if (method == "var_info") {
##         return(VAR_INFO)
##     }
##     if (method == "nvi") {
##         return(NVI)
##     }
## }
## <bytecode: 0x000001c58be00860>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{Clara\_Medoids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, clusters, samples, sample_size, distance_metric = "euclidean", 
##     minkowski_p = 1, threads = 1, swap_phase = TRUE, fuzzy = FALSE, 
##     verbose = FALSE, seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!is.numeric(clusters) || length(clusters) != 1 || clusters < 
##         1) 
##         stop("clusters should be numeric and greater than 0")
##     if (!is.numeric(samples) || length(samples) != 1 || samples < 
##         1) 
##         stop("samples should be a numeric value greater than 0")
##     if (!is.numeric(sample_size) || sample_size <= 0 || sample_size > 
##         1) 
##         stop("sample_size should be a numeric value greater than 0.0 and less than or equal to 1.0")
##     if (!distance_metric %in% c("euclidean", "manhattan", "chebyshev", 
##         "canberra", "braycurtis", "pearson_correlation", "simple_matching_coefficient", 
##         "minkowski", "hamming", "jaccard_coefficient", "Rao_coefficient", 
##         "mahalanobis", "cosine")) 
##         stop("the distance_metric should be one of 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient',\n         'minkowski', 'hamming', 'jaccard_coefficient', 'Rao_coefficient', 'mahalanobis', 'cosine'")
##     if (distance_metric == "minkowski" && minkowski_p == 0) 
##         stop("if distance metric is minkowski then the minkowski_p should be either a positive or a negative number but not 0.0")
##     if (threads < 1) 
##         stop("threads should be an integer greater than 0")
##     if (!is.logical(verbose)) 
##         stop("verbose should be either TRUE or FALSE")
##     if (!is.logical(swap_phase)) 
##         stop("swap_phase should be either TRUE or FALSE")
##     if (!is.logical(fuzzy)) 
##         stop("fuzzy should be either TRUE or FALSE")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     medoids_mat = ClaraMedoids(data, clusters, distance_metric, 
##         samples, sample_size, minkowski_p, threads, verbose, 
##         swap_phase, fuzzy, seed)
##     if (clusters > 1) {
##         dsm = data.frame(medoids_mat$silhouette_matrix)
##         colnames(dsm) = c("clusters", "neighbor_clusters", "intra_clust_dissim", 
##             "outer_clust_dissim", "silhouette_widths", "diameter", 
##             "separation")
##     }
##     else {
##         dsm = NULL
##     }
##     cs = data.frame(medoids_mat$clustering_stats)
##     colnames(cs) = c("clusters", "number_obs", "max_dissimilarity", 
##         "average_dissimilarity", "isolation")
##     cs$clusters = cs$clusters + 1
##     return(structure(list(call = match.call(), medoids = medoids_mat$medoids, 
##         medoid_indices = as.vector(medoids_mat$medoid_indices) + 
##             1, sample_indices = as.vector(medoids_mat$sample_indices) + 
##             1, best_dissimilarity = medoids_mat$best_dissimilarity, 
##         clusters = as.vector(medoids_mat$clusters) + 1, silhouette_matrix = dsm, 
##         fuzzy_probs = medoids_mat$fuzzy_probs, clustering_stats = cs, 
##         dissimilarity_matrix = medoids_mat$dissimilarity_matrix, 
##         distance_metric = distance_metric), class = c("MedoidsCluster", 
##         "cluster medoids silhouette")))
## }
## <bytecode: 0x000001c58be8c168>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{Cluster\_Medoids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, clusters, distance_metric = "euclidean", minkowski_p = 1, 
##     threads = 1, swap_phase = TRUE, fuzzy = FALSE, verbose = FALSE, 
##     seed = 1) 
## {
##     if (lifecycle::is_present(seed)) {
##         lifecycle::deprecate_warn(when = "1.2.6", what = "Cluster_Medoids(seed)", 
##             details = "The 'seed' parameter will be removed in version 1.3.0")
##     }
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame or a dissimilarity matrix with equal number of rows and columns and a diagonal equal to 0.0")
##     if (!is.numeric(clusters) || length(clusters) != 1 || clusters < 
##         1) 
##         stop("clusters should be numeric and greater than 0")
##     if (!distance_metric %in% c("euclidean", "manhattan", "chebyshev", 
##         "canberra", "braycurtis", "pearson_correlation", "simple_matching_coefficient", 
##         "minkowski", "hamming", "jaccard_coefficient", "Rao_coefficient", 
##         "mahalanobis", "cosine")) 
##         stop("the distance_metric should be one of 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient',\n         'minkowski', 'hamming', 'jaccard_coefficient', 'Rao_coefficient', 'mahalanobis', 'cosine'")
##     if (distance_metric == "minkowski" && minkowski_p == 0) 
##         stop("if distance metric is minkowski then the minkowski_p should be either a positive or a negative number but not 0.0")
##     if (threads < 1) 
##         stop("threads should be an integer greater than 0")
##     if (!is.logical(swap_phase)) 
##         stop("swap_phase should be either TRUE or FALSE")
##     if (!is.logical(fuzzy)) 
##         stop("fuzzy should be either TRUE or FALSE")
##     if (!is.logical(verbose)) 
##         stop("verbose should be either TRUE or FALSE")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     medoids_mat = ClusterMedoids(data, clusters, distance_metric, 
##         minkowski_p, threads, verbose, swap_phase, fuzzy, seed)
##     if (clusters > 1) {
##         dsm = data.frame(medoids_mat$silhouette_matrix)
##         colnames(dsm) = c("clusters", "neighbor_clusters", "intra_clust_dissim", 
##             "outer_clust_dissim", "silhouette_widths", "diameter", 
##             "separation")
##         cs = data.frame(medoids_mat$clustering_stats)
##         colnames(cs) = c("clusters", "number_obs", "max_dissimilarity", 
##             "average_dissimilarity", "diameter", "separation")
##     }
##     else {
##         dsm = NULL
##         cs = NULL
##     }
##     if (medoids_mat$flag_dissim_mat) {
##         tmp_rows = as.vector(medoids_mat$medoids) + 1
##     }
##     else {
##         tmp_rows = data[as.vector(medoids_mat$medoids) + 1, ]
##     }
##     return(structure(list(call = match.call(), medoids = tmp_rows, 
##         medoid_indices = as.vector(medoids_mat$medoids) + 1, 
##         best_dissimilarity = medoids_mat$cost, dissimilarity_matrix = medoids_mat$dissimilarity_matrix, 
##         clusters = as.vector(medoids_mat$clusters) + 1, silhouette_matrix = dsm, 
##         fuzzy_probs = medoids_mat$fuzzy_probs, clustering_stats = cs, 
##         distance_metric = distance_metric), class = c("MedoidsCluster", 
##         "cluster medoids silhouette")))
## }
## <bytecode: 0x000001c58beeb818>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{GMM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, gaussian_comps = 1, dist_mode = "eucl_dist", 
##     seed_mode = "random_subset", km_iter = 10, em_iter = 5, verbose = FALSE, 
##     var_floor = 1e-10, seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (gaussian_comps < 1) 
##         stop("the number of gaussian mixture components should be greater than 0")
##     if (!dist_mode %in% c("eucl_dist", "maha_dist")) 
##         stop("available distance modes are 'eucl_dist' and 'maha_dist'")
##     if (!seed_mode %in% c("static_subset", "random_subset", "static_spread", 
##         "random_spread")) 
##         stop("available seed modes are 'static_subset','random_subset','static_spread' and 'random_spread'")
##     if (km_iter < 0) 
##         stop("the km_iter parameter can not be negative")
##     if (em_iter < 0) 
##         stop("the em_iter parameter can not be negative")
##     if (!is.logical(verbose)) 
##         stop("the verbose parameter should be either TRUE or FALSE")
##     if (var_floor < 0) 
##         stop("the var_floor parameter can not be negative")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = tryCatch_GMM(data, gaussian_comps, dist_mode, seed_mode, 
##         km_iter, em_iter, verbose, var_floor, seed)
##     if ("Error" %in% names(res)) {
##         return(res)
##     }
##     else {
##         structure(list(call = match.call(), centroids = res$centroids, 
##             covariance_matrices = res$covariance_matrices, weights = as.vector(res$weights), 
##             Log_likelihood = res$Log_likelihood_raw), class = c("GMMCluster", 
##             "Gaussian Mixture Models"))
##     }
## }
## <bytecode: 0x000001c58bf470b8>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{KMeans\_arma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, clusters, n_iter = 10, seed_mode = "random_subset", 
##     verbose = FALSE, CENTROIDS = NULL, seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!is.numeric(clusters) || length(clusters) != 1 || clusters < 
##         1) 
##         stop("clusters should be numeric and greater than 0")
##     if (n_iter < 0) 
##         stop("the n_iter parameter can not be negative")
##     if (!seed_mode %in% c("keep_existing", "static_subset", "random_subset", 
##         "static_spread", "random_spread")) 
##         stop("available seed modes are 'keep_existing','static_subset','random_subset','static_spread' and 'random_spread'")
##     if ((seed_mode == "keep_existing" && is.null(CENTROIDS)) || 
##         (seed_mode != "keep_existing" && !is.null(CENTROIDS))) 
##         stop("the keep_existing seed_mode should be used when CENTROIDS is not NULL")
##     if (!is.logical(verbose)) 
##         stop("the verbose parameter should be either TRUE or FALSE")
##     if (!is.null(CENTROIDS) && (!inherits(CENTROIDS, "matrix") || 
##         nrow(CENTROIDS) != clusters || ncol(CENTROIDS) != ncol(data))) 
##         stop("CENTROIDS should be a matrix with number of rows equal to the number of clusters and number of columns equal to the number of columns of the data")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = tryCatch_KMEANS_arma(data, clusters, n_iter, verbose, 
##         seed_mode, CENTROIDS, seed)
##     if ("Error" %in% names(res) || is.character(res)) {
##         return(res)
##     }
##     else {
##         return(structure(res, class = "k-means clustering"))
##     }
## }
## <bytecode: 0x000001c58bfa7500>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{KMeans\_rcpp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, clusters, num_init = 1, max_iters = 100, initializer = "kmeans++", 
##     fuzzy = FALSE, verbose = FALSE, CENTROIDS = NULL, tol = 1e-04, 
##     tol_optimal_init = 0.3, seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!is.numeric(clusters) || length(clusters) != 1 || clusters < 
##         1) 
##         stop("clusters should be numeric and greater than 0")
##     if (num_init < 1) 
##         stop("the num_init parameter should be greater than 0")
##     if (max_iters < 1) 
##         stop("the max_iters parameter should be greater than 0")
##     if (!initializer %in% c("kmeans++", "random", "optimal_init", 
##         "quantile_init")) 
##         stop("available initializer methods are 'kmeans++', 'random', 'optimal_init' and 'quantile_init'")
##     if (!is.logical(fuzzy)) 
##         stop("the fuzzy parameter should be either TRUE or FALSE")
##     if (!is.logical(verbose)) 
##         stop("the verbose parameter should be either TRUE or FALSE")
##     if (!is.null(CENTROIDS) && (!inherits(CENTROIDS, "matrix") || 
##         nrow(CENTROIDS) != clusters || ncol(CENTROIDS) != ncol(data))) 
##         stop("CENTROIDS should be a matrix with number of rows equal to the number of clusters and number of columns equal to the number of columns of the data")
##     if (tol <= 0) 
##         stop("tol should be a float number greater than 0.0")
##     if (tol_optimal_init <= 0) 
##         stop("tol_optimal_init should be a float number greater than 0.0")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = KMEANS_rcpp(data, clusters, num_init, max_iters, initializer, 
##         fuzzy, verbose, CENTROIDS, tol, eps = 1e-06, tol_optimal_init, 
##         seed)
##     if (fuzzy) {
##         return(structure(list(call = match.call(), clusters = as.vector(res$clusters + 
##             1), fuzzy_clusters = res$fuzzy_clusters, centroids = res$centers, 
##             total_SSE = res$total_SSE, best_initialization = res$best_initialization, 
##             WCSS_per_cluster = res$WCSS_per_cluster, obs_per_cluster = res$obs_per_cluster, 
##             between.SS_DIV_total.SS = (res$total_SSE - sum(res$WCSS_per_cluster))/res$total_SSE), 
##             class = c("KMeansCluster", "k-means clustering")))
##     }
##     else {
##         return(structure(list(call = match.call(), clusters = as.vector(res$clusters + 
##             1), centroids = res$centers, total_SSE = res$total_SSE, 
##             best_initialization = res$best_initialization, WCSS_per_cluster = res$WCSS_per_cluster, 
##             obs_per_cluster = res$obs_per_cluster, between.SS_DIV_total.SS = (res$total_SSE - 
##                 sum(res$WCSS_per_cluster))/res$total_SSE), class = c("KMeansCluster", 
##             "k-means clustering")))
##     }
## }
## <bytecode: 0x000001c58c00ab08>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{MiniBatchKmeans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, clusters, batch_size = 10, num_init = 1, max_iters = 100, 
##     init_fraction = 1, initializer = "kmeans++", early_stop_iter = 10, 
##     verbose = FALSE, CENTROIDS = NULL, tol = 1e-04, tol_optimal_init = 0.3, 
##     seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!is.numeric(clusters) || length(clusters) != 1 || clusters < 
##         1) 
##         stop("clusters should be numeric and greater than 0")
##     if (batch_size < 1) 
##         stop("batch_size should be greater than 0")
##     if (num_init < 1) 
##         stop("the num_init parameter should be greater than 0")
##     if (max_iters < 1) 
##         stop("the max_iters parameter should be greater than 0")
##     if (init_fraction <= 0 || init_fraction > 1) 
##         stop("init_fraction is a float greater than 0 and less or equal to 1.0")
##     if (!initializer %in% c("kmeans++", "random", "optimal_init", 
##         "quantile_init")) 
##         stop("available initializer methods are 'kmeans++', 'random', 'optimal_init' and 'quantile_init'")
##     if (early_stop_iter < 1) 
##         stop("early_stop_iter should be greater than 0")
##     if (!is.logical(verbose)) 
##         stop("the verbose parameter should be either TRUE or FALSE")
##     if (!is.null(CENTROIDS) && (!inherits(CENTROIDS, "matrix") || 
##         nrow(CENTROIDS) != clusters || ncol(CENTROIDS) != ncol(data))) 
##         stop("CENTROIDS should be a matrix with number of rows equal to the number of clusters and number of columns equal to the number of columns of the data")
##     if (tol <= 0) 
##         stop("tol should be a float number greater than 0.0")
##     if (tol_optimal_init <= 0) 
##         stop("tol_optimal_init should be a float number greater than 0.0")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = mini_batch_kmeans(data, clusters, batch_size, max_iters, 
##         num_init, init_fraction, initializer, early_stop_iter, 
##         verbose, CENTROIDS, tol, tol_optimal_init, seed)
##     structure(res, class = c("KMeansCluster", "k-means clustering"))
## }
## <bytecode: 0x000001c58c0646b0>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{Optimal\_Clusters\_GMM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, max_clusters, criterion = "AIC", dist_mode = "eucl_dist", 
##     seed_mode = "random_subset", km_iter = 10, em_iter = 5, verbose = FALSE, 
##     var_floor = 1e-10, plot_data = TRUE, seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!inherits(max_clusters, c("numeric", "integer"))) 
##         stop("max_clusters should be a numeric or integer vector")
##     if (length(max_clusters) == 1) {
##         if (plot_data && max_clusters < 2) 
##             stop("if plot_data is TRUE the max_clusters parameter should be at least 2")
##     }
##     if (!criterion %in% c("AIC", "BIC")) 
##         stop("supported criteria are 'AIC' or 'BIC'")
##     if (!dist_mode %in% c("eucl_dist", "maha_dist")) 
##         stop("available distance modes are 'eucl_dist' and 'maha_dist'")
##     if (!seed_mode %in% c("static_subset", "random_subset", "static_spread", 
##         "random_spread")) 
##         stop("available seed modes are 'static_subset','random_subset','static_spread' and 'random_spread'")
##     if (km_iter < 0) 
##         stop("the km_iter parameter can not be negative")
##     if (em_iter < 0) 
##         stop("the em_iter parameter can not be negative")
##     if (!is.logical(verbose)) 
##         stop("the verbose parameter should be either TRUE or FALSE")
##     if (var_floor < 0) 
##         stop("the var_floor parameter can not be negative")
##     if (length(max_clusters) != 1) {
##         plot_data = FALSE
##         if (ncol(data) < max(max_clusters) && verbose) {
##             warning("the number of columns of the data should be larger than the maximum value of 'max_clusters'", 
##                 call. = F)
##             cat(" ", "\n")
##         }
##     }
##     else {
##         if (ncol(data) < max_clusters && verbose) {
##             warning("the number of columns of the data should be larger than 'max_clusters'", 
##                 call. = F)
##             cat(" ", "\n")
##         }
##     }
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     if (length(max_clusters) == 1) {
##         pass_vector = 1:max_clusters
##     }
##     else {
##         pass_vector = max_clusters
##     }
##     if (0 %in% pass_vector) {
##         stop("The 'max_clusters' vector can not include a 0 value !", 
##             call. = F)
##     }
##     gmm = tryCatch_optimal_clust_GMM(data, pass_vector, dist_mode, 
##         seed_mode, km_iter, em_iter, verbose, var_floor, criterion, 
##         seed)
##     if ("Error" %in% names(gmm)) {
##         return(gmm)
##     }
##     else {
##         if (plot_data) {
##             if (grDevices::dev.cur() != 1) {
##                 grDevices::dev.off()
##             }
##             vec_out = as.vector(gmm)
##             tmp_VAL = as.vector(stats::na.omit(vec_out))
##             if (length(which(is.na(vec_out))) > 0) {
##                 x_dis = (1:length(vec_out))[-which(is.na(vec_out))]
##                 y_dis = vec_out[-which(is.na(vec_out))]
##             }
##             else {
##                 x_dis = 1:length(vec_out)
##                 y_dis = vec_out
##             }
##             y_MAX = max(tmp_VAL)
##             graphics::plot(x = x_dis, y = y_dis, type = "l", 
##                 xlab = "clusters", ylab = criterion, col = "blue", 
##                 lty = 3, axes = FALSE)
##             graphics::axis(1, at = seq(1, length(vec_out), by = 1))
##             graphics::axis(2, at = seq(round(min(tmp_VAL) - round(summary(y_MAX)[["Max."]])/10), 
##                 y_MAX + round(summary(y_MAX)[["Max."]])/10, by = round((summary(tmp_VAL)["Max."] - 
##                   summary(tmp_VAL)["Min."])/5)), las = 1, cex.axis = 0.8)
##             graphics::abline(h = seq(round(min(tmp_VAL) - round(summary(y_MAX)[["Max."]])/10), 
##                 y_MAX + round(summary(y_MAX)[["Max."]])/10, by = round((summary(tmp_VAL)["Max."] - 
##                   summary(tmp_VAL)["Min."])/5)), v = seq(1, length(vec_out), 
##                 by = 1), col = "gray", lty = 3)
##             graphics::text(x = 1:length(vec_out), y = vec_out, 
##                 labels = round(vec_out, 1), cex = 0.8, font = 2)
##         }
##         res = as.vector(gmm)
##         return(res)
##     }
## }
## <bytecode: 0x000001c58c0db478>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{Optimal\_Clusters\_KMeans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, max_clusters, criterion = "variance_explained", 
##     fK_threshold = 0.85, num_init = 1, max_iters = 200, initializer = "kmeans++", 
##     tol = 1e-04, plot_clusters = TRUE, verbose = FALSE, tol_optimal_init = 0.3, 
##     seed = 1, mini_batch_params = NULL) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!inherits(max_clusters, c("numeric", "integer"))) 
##         stop("max_clusters should be a numeric or integer vector")
##     if (length(max_clusters) == 1) {
##         if (max_clusters < 1) {
##             stop("In case that max_clusters is of length 1 it should be greater than 0")
##         }
##     }
##     if (!criterion %in% c("variance_explained", "WCSSE", "dissimilarity", 
##         "silhouette", "distortion_fK", "AIC", "BIC", "Adjusted_Rsquared")) 
##         stop("available criteria are 'variance_explained', 'WCSSE', 'dissimilarity', 'silhouette', 'distortion_fK', 'AIC', 'BIC' and 'Adjusted_Rsquared'")
##     if (num_init < 1) 
##         stop("the num_init parameter should be greater than 0")
##     if (max_iters < 1) 
##         stop("the max_iters parameter should be greater than 0")
##     if (!initializer %in% c("kmeans++", "random", "optimal_init", 
##         "quantile_init")) 
##         stop("available initializer methods are 'kmeans++', 'random', 'quantile_init' and 'optimal_init'")
##     if (tol <= 0) 
##         stop("tol should be a float number greater than 0.0")
##     if (!is.logical(plot_clusters)) 
##         stop("the plot_clusters parameter should be either TRUE or FALSE")
##     if (!is.logical(verbose)) 
##         stop("the verbose parameter should be either TRUE or FALSE")
##     if (tol_optimal_init <= 0) 
##         stop("tol_optimal_init should be a float number greater than 0.0")
##     if (!is.null(mini_batch_params)) {
##         if (!all(names(mini_batch_params) %in% c("batch_size", 
##             "init_fraction", "early_stop_iter"))) {
##             stop("The 'mini_batch_params' parameter should be of type list and valid inputs to the 'mini_batch_params' are: 'batch_size', 'init_fraction' and 'early_stop_iter'!", 
##                 call. = F)
##         }
##         if (criterion == "variance_explained") {
##             stop("The 'variance_explained' criterion is not supported in case of mini-batch-kmeans (when 'mini_batch_params' is not NULL)!", 
##                 call. = F)
##         }
##     }
##     if (length(max_clusters) != 1) 
##         plot_clusters = FALSE
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     LEN_CLUST = ITER_CLUST = NA
##     if (length(max_clusters) == 1) {
##         LEN_CLUST = max_clusters
##         ITER_CLUST = 1:max_clusters
##     }
##     else {
##         LEN_CLUST = length(max_clusters)
##         ITER_CLUST = max_clusters
##     }
##     vec_out = rep(NA, LEN_CLUST)
##     if (verbose) {
##         cat("", "\n")
##         pb = utils::txtProgressBar(min = 1, max = LEN_CLUST, 
##             style = 3)
##         cat("", "\n")
##     }
##     COUNT = 1
##     for (i in ITER_CLUST) {
##         if (is.null(mini_batch_params)) {
##             km = KMEANS_rcpp(data, i, num_init, max_iters, initializer, 
##                 FALSE, FALSE, NULL, tol, 1e-06, tol_optimal_init, 
##                 seed)
##         }
##         else {
##             km = MiniBatchKmeans(data, i, mini_batch_params[["batch_size"]], 
##                 num_init, max_iters, mini_batch_params[["init_fraction"]], 
##                 initializer, mini_batch_params[["early_stop_iter"]], 
##                 FALSE, NULL, tol, tol_optimal_init, seed)
##             tmp_cent = km$centroids
##             km["centroids"] = NULL
##             km[["centers"]] = tmp_cent
##             if (criterion %in% c("dissimilarity", "silhouette", 
##                 "BIC")) {
##                 km_preds = predict_MBatchKMeans(data, tmp_cent, 
##                   FALSE)
##                 km[["clusters"]] = as.vector(km_preds)
##             }
##         }
##         if (criterion == "variance_explained") {
##             vec_out[COUNT] = sum(stats::na.omit(as.vector(km$WCSS_per_cluster)))/km$total_SSE
##         }
##         if (criterion == "WCSSE") {
##             vec_out[COUNT] = sum(stats::na.omit(as.vector(km$WCSS_per_cluster)))
##         }
##         if (criterion == "dissimilarity") {
##             eval_km = evaluation_rcpp(data, as.vector(km$clusters), 
##                 FALSE)
##             tmp_dis = mean(stats::na.omit(unlist(lapply(eval_km$INTRA_cluster_dissimilarity, 
##                 mean))))
##             vec_out[COUNT] = tmp_dis
##         }
##         if (criterion == "silhouette") {
##             if (i == 1) {
##                 vec_out[COUNT] = 0
##             }
##             else {
##                 eval_km = evaluation_rcpp(data, as.vector(km$clusters), 
##                   TRUE)
##                 tmp_silh = mean(stats::na.omit(unlist(lapply(eval_km$silhouette, 
##                   mean))))
##                 vec_out[COUNT] = tmp_silh
##             }
##         }
##         if (criterion == "distortion_fK") {
##             vec_out[COUNT] = sum(stats::na.omit(as.vector(km$WCSS_per_cluster)))
##         }
##         if (criterion == "AIC") {
##             m = ncol(km$centers)
##             k = nrow(km$centers)
##             D = sum(stats::na.omit(km$WCSS_per_cluster))
##             vec_out[COUNT] = D + 2 * m * k
##         }
##         if (criterion == "BIC") {
##             m = ncol(km$centers)
##             k = nrow(km$centers)
##             n = length(km$clusters)
##             D = sum(stats::na.omit(km$WCSS_per_cluster))
##             vec_out[COUNT] = D + log(n) * m * k
##         }
##         if (criterion == "Adjusted_Rsquared") {
##             vec_out[COUNT] = sum(stats::na.omit(km$WCSS_per_cluster))
##         }
##         if (verbose) {
##             utils::setTxtProgressBar(pb, COUNT)
##         }
##         COUNT = COUNT + 1
##     }
##     if (verbose) {
##         close(pb)
##         cat("", "\n")
##     }
##     if (criterion == "Adjusted_Rsquared") {
##         if (length(max_clusters) != 1) {
##             vec_out = "The 'Adjusted_Rsquared' criterion doesn't return the correct output if the 'max_clusters' parameter is greater than 1"
##         }
##         else {
##             vec_out = 1 - (vec_out * (nrow(data) - 1))/(vec_out[1] * 
##                 (nrow(data) - ITER_CLUST))
##         }
##     }
##     if (criterion %in% c("variance_explained", "WCSSE", "dissimilarity", 
##         "silhouette", "AIC", "BIC", "Adjusted_Rsquared")) {
##         if (plot_clusters) {
##             tmp_VAL = as.vector(stats::na.omit(vec_out))
##             if (length(which(is.na(vec_out))) > 0) {
##                 x_dis = (1:length(vec_out))[-which(is.na(vec_out))]
##                 y_dis = vec_out[-which(is.na(vec_out))]
##             }
##             else {
##                 x_dis = 1:length(vec_out)
##                 y_dis = vec_out
##             }
##             y_MAX = max(tmp_VAL)
##             graphics::plot(x = x_dis, y = y_dis, type = "l", 
##                 xlab = "clusters", ylab = criterion, col = "blue", 
##                 lty = 3, axes = FALSE)
##             graphics::axis(1, at = seq(1, length(vec_out), by = 1))
##             if (criterion == "silhouette") {
##                 graphics::axis(2, at = seq(0, y_MAX + 0.05, by = 0.05), 
##                   las = 1, cex.axis = 0.8)
##                 graphics::abline(h = seq(0, max(as.vector(stats::na.omit(vec_out))), 
##                   0.05), v = seq(1, length(vec_out), by = 1), 
##                   col = "gray", lty = 3)
##             }
##             else {
##                 tmp_summary = round(summary(y_MAX)[["Max."]])
##                 out_max_summary = ifelse(tmp_summary == 0, 1, 
##                   tmp_summary)
##                 graphics::axis(2, at = seq(0, y_MAX + out_max_summary/10, 
##                   by = out_max_summary/10), las = 1, cex.axis = 0.8)
##                 graphics::abline(h = seq(0, max(as.vector(stats::na.omit(vec_out))), 
##                   out_max_summary/10), v = seq(1, length(vec_out), 
##                   by = 1), col = "gray", lty = 3)
##             }
##             if (criterion %in% c("variance_explained", "Adjusted_Rsquared", 
##                 "dissimilarity", "silhouette")) {
##                 graphics::text(x = 1:length(vec_out), y = vec_out, 
##                   labels = round(vec_out, 2), cex = 0.8, font = 2)
##             }
##             else {
##                 graphics::text(x = 1:length(vec_out), y = vec_out, 
##                   labels = round(vec_out, 1), cex = 0.8, font = 2)
##             }
##         }
##     }
##     else {
##         if (length(max_clusters) != 1) {
##             fK_vec = "The 'distortion_fK' criterion can not be computed if the length of the 'max_clusters' parameter is greater than 1. See the details for more information!"
##         }
##         else {
##             f_K = opt_clust_fK(vec_out, ncol(data), fK_threshold)
##             fK_vec = as.vector(f_K$fK_evaluation)
##         }
##         if (plot_clusters) {
##             if (length(which(is.na(fK_vec))) > 0) {
##                 x_fk = (1:length(fK_vec))[-which(is.na(fK_vec))]
##                 y_fk = fK_vec[-which(is.na(fK_vec))]
##             }
##             else {
##                 x_fk = 1:length(fK_vec)
##                 y_fk = fK_vec
##             }
##             graphics::par(oma = c(0, 2, 0, 0))
##             graphics::plot(y_fk, type = "l", xlab = "clusters", 
##                 ylab = "f(K)", col = "green", axes = FALSE)
##             graphics::axis(1, at = x_fk)
##             graphics::axis(2, at = seq(0, max(y_fk) + 0.1, by = round(summary(y_fk)[["Max."]])/10), 
##                 las = 1, cex.axis = 0.8)
##             graphics::abline(h = seq(0, max(y_fk), round(summary(y_fk)[["Max."]])/10), 
##                 v = seq(1, length(y_fk), by = 1), col = "gray", 
##                 lty = 3)
##             graphics::abline(h = fK_threshold, col = "blue", 
##                 lty = 3)
##             graphics::mtext("threshold", side = 2, line = 2, 
##                 at = fK_threshold, las = 1, cex = 0.9)
##             graphics::text(x = x_fk, y = y_fk, labels = round(y_fk, 
##                 2), cex = 0.8, font = 2)
##         }
##     }
##     if (criterion %in% c("variance_explained", "WCSSE", "dissimilarity", 
##         "silhouette", "AIC", "BIC", "Adjusted_Rsquared")) {
##         return(vec_out)
##     }
##     else {
##         return(fK_vec)
##     }
## }
## <bytecode: 0x000001c58c12ec58>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{Optimal\_Clusters\_Medoids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, max_clusters, distance_metric, criterion = "dissimilarity", 
##     clara_samples = 0, clara_sample_size = 0, minkowski_p = 1, 
##     swap_phase = TRUE, threads = 1, verbose = FALSE, plot_clusters = TRUE, 
##     seed = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!inherits(max_clusters, c("numeric", "integer"))) 
##         stop("max_clusters should be a numeric or integer vector")
##     if (length(max_clusters) == 1) {
##         if (max_clusters < 1) {
##             stop("In case that max_clusters is of length 1 it should be greater than 0")
##         }
##     }
##     if (!distance_metric %in% c("euclidean", "manhattan", "chebyshev", 
##         "canberra", "braycurtis", "pearson_correlation", "simple_matching_coefficient", 
##         "minkowski", "hamming", "jaccard_coefficient", "Rao_coefficient", 
##         "mahalanobis", "cosine")) 
##         stop("the distance_metric should be one of 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient',\n         'minkowski', 'hamming', 'jaccard_coefficient', 'Rao_coefficient', 'mahalanobis', 'cosine'")
##     if (!criterion %in% c("silhouette", "dissimilarity")) 
##         stop("supported criteria are 'silhouette' and 'dissimilarity'")
##     if (distance_metric == "minkowski" && minkowski_p == 0) 
##         stop("if distance metric is minkowski then the minkowski_p should be either a positive or a negative number but not 0.0")
##     if (!is.logical(swap_phase)) 
##         stop("swap_phase should be either TRUE or FALSE")
##     if (threads < 1) 
##         stop("threads should be an integer greater than 0")
##     if (!is.logical(verbose)) 
##         stop("verbose should be either TRUE or FALSE")
##     if (!is.logical(plot_clusters)) 
##         stop("plot_clusters should be either TRUE or FALSE")
##     if (clara_samples != 0 && (!is.numeric(clara_samples) || 
##         length(clara_samples) != 1 || clara_samples < 1)) 
##         stop("clara_samples should be a numeric value greater than 0")
##     if (clara_sample_size != 0 && (!is.numeric(clara_sample_size) || 
##         clara_sample_size < 0 || clara_sample_size > 1)) 
##         stop("clara_sample_size should be a numeric value greater than 0.0 and less than or equal to 1.0")
##     if ((clara_samples > 0 && clara_sample_size == 0) || (clara_samples == 
##         0 && clara_sample_size > 0)) 
##         stop("to run clustering for large applications (clara) both 'clara_samples' and 'clara_sample_size' should be greater than 0")
##     if (clara_samples > 0 && clara_sample_size > 0 && sum(diag(data)) == 
##         0 && nrow(data) == ncol(data)) 
##         stop("a dissimilarity matrix is only allowed for the 'Cluster_Medoids' function")
##     if (length(max_clusters) != 1) 
##         plot_clusters = FALSE
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     if (length(max_clusters) == 1) {
##         pass_vector = 1:max_clusters
##     }
##     else {
##         pass_vector = max_clusters
##     }
##     if (0 %in% pass_vector) {
##         stop("The 'max_clusters' vector can not include a 0 value !", 
##             call. = F)
##     }
##     inter_bool = ifelse(criterion == "silhouette", TRUE, FALSE)
##     if (clara_samples > 0 && clara_sample_size > 0) {
##         opt_cl = OptClust(data, pass_vector, distance_metric, 
##             TRUE, clara_samples, clara_sample_size, minkowski_p, 
##             criterion, threads, swap_phase, verbose, seed)
##     }
##     else {
##         opt_cl = OptClust(data, pass_vector, distance_metric, 
##             FALSE, clara_samples, clara_sample_size, minkowski_p, 
##             criterion, threads, swap_phase, verbose, seed)
##     }
##     if (plot_clusters) {
##         if (grDevices::dev.cur() != 1) {
##             grDevices::dev.off()
##         }
##         if (criterion == "dissimilarity") {
##             tmp_dis = rep(NA, max_clusters)
##             for (i in 2:max_clusters) {
##                 tmp_dis[i] = opt_cl[[i]]$avg_intra_clust_dissimilarity
##             }
##             SUM_dis = sum(stats::na.omit(tmp_dis))
##             for (i in 2:max_clusters) {
##                 tmp_dis[i] = tmp_dis[i]/SUM_dis
##             }
##             tmp_VAL = as.vector(stats::na.omit(tmp_dis))
##             if (length(which(is.na(tmp_dis))) > 0) {
##                 x_dis = (1:length(tmp_dis))[-which(is.na(tmp_dis))]
##                 y_dis = tmp_dis[-which(is.na(tmp_dis))]
##             }
##             else {
##                 x_dis = 1:length(tmp_dis)
##                 y_dis = tmp_dis
##             }
##             graphics::plot(x = x_dis, y = y_dis, type = "l", 
##                 xlab = "clusters", ylab = criterion, col = "red", 
##                 xaxp = c(1, 10, 10), axes = FALSE)
##             graphics::axis(1, at = seq(1, length(tmp_dis), by = 1))
##             graphics::axis(2, at = round(seq(0, max(tmp_VAL) + 
##                 max(tmp_VAL)/10, by = 0.01), 2))
##             graphics::abline(h = seq(0, max(tmp_VAL) + max(tmp_VAL)/10, 
##                 0.1), v = seq(1, length(tmp_dis), by = 1), col = "gray", 
##                 lty = 3)
##             graphics::text(x = x_dis, y = y_dis, labels = round(y_dis, 
##                 3), cex = 0.8)
##             graphics::legend("topright", legend = "avg. dissimilarity", 
##                 col = "red", lty = 1, text.font = 1)
##             try_c = tryCatch(function_interactive(opt_cl, max_clusters, 
##                 inter_bool), error = function(e) e)
##             if (inherits(try_c, "error")) {
##                 warning("The plot can not be created for the specified number of clusters. This means the output data do not fit in the figure (plot) margins.", 
##                   call. = F)
##             }
##             else {
##                 try_c
##             }
##         }
##         if (criterion == "silhouette") {
##             tmp_dis = rep(NA, max_clusters)
##             for (i in 2:max_clusters) {
##                 tmp_dis[i] = opt_cl[[i]]$avg_intra_clust_dissimilarity
##             }
##             SUM_dis = sum(stats::na.omit(tmp_dis))
##             for (i in 2:max_clusters) {
##                 tmp_dis[i] = tmp_dis[i]/SUM_dis
##             }
##             if (length(which(is.na(tmp_dis))) > 0) {
##                 x_dis = (1:length(tmp_dis))[-which(is.na(tmp_dis))]
##                 y_dis = tmp_dis[-which(is.na(tmp_dis))]
##             }
##             else {
##                 x_dis = 1:length(tmp_dis)
##                 y_dis = tmp_dis
##             }
##             tmp_silh = rep(NA, max_clusters)
##             for (i in 2:max_clusters) {
##                 tmp_silh[i] = opt_cl[[i]]$avg_width_silhouette
##             }
##             SUM_sil = sum(stats::na.omit(tmp_silh))
##             for (i in 2:max_clusters) {
##                 tmp_silh[i] = tmp_silh[i]/SUM_sil
##             }
##             if (length(which(is.na(tmp_silh))) > 0) {
##                 x_sil = (1:length(tmp_silh))[-which(is.na(tmp_silh))]
##                 y_sil = tmp_silh[-which(is.na(tmp_silh))]
##             }
##             else {
##                 x_sil = 1:length(tmp_silh)
##                 y_sil = tmp_silh
##             }
##             tmp_VAL_ALL = as.vector(stats::na.omit(c(tmp_dis, 
##                 tmp_silh)))
##             y_MIN = min(tmp_VAL_ALL)
##             y_MAX = max(tmp_VAL_ALL)
##             graphics::plot(x = x_dis, y = y_dis, type = "l", 
##                 xlab = "clusters", ylim = c(y_MIN, y_MAX), col = "red", 
##                 ylab = "dissimilarity -- silhouette", axes = FALSE)
##             graphics::axis(1, at = seq(1, length(tmp_dis), by = 1))
##             graphics::axis(2, at = round(seq(y_MIN, y_MAX + y_MAX/10, 
##                 by = 0.01), 2))
##             graphics::abline(h = seq(0, y_MAX + y_MAX/10, 0.05), 
##                 v = seq(1, length(tmp_dis), by = 1), col = "gray", 
##                 lty = 3)
##             graphics::text(x = x_dis, y = y_dis, labels = round(y_dis, 
##                 3), cex = 0.8)
##             graphics::lines(x = x_sil[1:length(x_sil)], y = y_sil[1:length(y_sil)], 
##                 type = "l", col = "blue")
##             graphics::text(x = x_sil[1:length(x_sil)], y = y_sil[1:length(y_sil)], 
##                 labels = round(y_sil[1:length(y_sil)], 3), cex = 0.8)
##             graphics::legend("topright", legend = c("avg. dissimilarity", 
##                 "avg. silhouette width"), col = c("red", "blue"), 
##                 lty = 1, text.font = 1)
##             try_c = tryCatch(function_interactive(opt_cl, max_clusters, 
##                 inter_bool), error = function(e) e)
##             if (inherits(try_c, "error")) {
##                 warning("The plot can not be created for the specified number of clusters. This means the output data do not fit in the figure (plot) margins.", 
##                   call. = F)
##             }
##             else {
##                 try_c
##             }
##         }
##     }
##     return(opt_cl)
## }
## <bytecode: 0x000001c58c1d19f8>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{predict\_GMM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, CENTROIDS, COVARIANCE, WEIGHTS) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if ("data.frame" %in% class(CENTROIDS)) 
##         CENTROIDS = as.matrix(CENTROIDS)
##     if (!inherits(CENTROIDS, "matrix")) 
##         stop("CENTROIDS should be either a matrix or a data frame")
##     if ("data.frame" %in% class(COVARIANCE)) 
##         COVARIANCE = as.matrix(COVARIANCE)
##     if (!inherits(COVARIANCE, "matrix")) 
##         stop("COVARIANCE should be either a matrix or a data frame")
##     if (ncol(data) != ncol(CENTROIDS) || ncol(data) != ncol(COVARIANCE) || 
##         length(WEIGHTS) != nrow(CENTROIDS) || length(WEIGHTS) != 
##         nrow(COVARIANCE)) 
##         stop("the number of columns of the data, CENTROIDS and COVARIANCE should match and the number of rows of the CENTROIDS AND COVARIANCE should be equal to the length of the WEIGHTS vector")
##     if (!inherits(WEIGHTS, "numeric") || !is.vector(WEIGHTS)) 
##         stop("WEIGHTS should be a numeric vector")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = predict_MGausDPDF(data, CENTROIDS, COVARIANCE, WEIGHTS, 
##         eps = 1e-08)
##     list(log_likelihood = res$Log_likelihood_raw, cluster_proba = res$cluster_proba, 
##         cluster_labels = as.vector(res$cluster_labels) + 1)
## }
## <bytecode: 0x000001c58c24fd48>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{predict\_KMeans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, CENTROIDS, threads = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!is.matrix(CENTROIDS)) 
##         stop("CENTROIDS should be a matrix")
##     if (ncol(data) != ncol(CENTROIDS)) 
##         stop("the number of columns of the data should match the number of columns of the CENTROIDS ")
##     if (threads < 1) 
##         stop("the number of threads should be greater or equal to 1")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values", call. = F)
##     if (!is.null(class(CENTROIDS))) 
##         class(CENTROIDS) = NULL
##     flag_dups = duplicated(CENTROIDS)
##     if (sum(flag_dups) > 0) 
##         stop("The 'CENTROIDS' input matrix includes duplicated rows!", 
##             call. = F)
##     as.vector(validate_centroids(data, CENTROIDS, threads)) + 
##         1
## }
## <bytecode: 0x000001c58c2a1cd0>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{predict\_MBatchKMeans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, CENTROIDS, fuzzy = FALSE) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if (!inherits(CENTROIDS, "matrix")) 
##         stop("CENTROIDS should be a matrix")
##     if (!(ncol(data) == ncol(CENTROIDS))) 
##         stop("the number of columns of the data should match the number of columns of the CENTROIDS ")
##     if (!is.logical(fuzzy)) 
##         stop("fuzzy should be either TRUE or FALSE")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = Predict_mini_batch_kmeans(data, CENTROIDS, fuzzy, eps = 1e-06)
##     if (fuzzy) {
##         return(structure(list(clusters = as.vector(res$clusters + 
##             1), fuzzy_clusters = res$fuzzy_clusters), class = "k-means clustering"))
##     }
##     else {
##         tmp_res = as.vector(res$clusters + 1)
##         return(tmp_res)
##     }
## }
## <bytecode: 0x000001c58c304b38>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{predict\_Medoids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (data, MEDOIDS = NULL, distance_metric = "euclidean", 
##     fuzzy = FALSE, minkowski_p = 1, threads = 1) 
## {
##     if ("data.frame" %in% class(data)) 
##         data = as.matrix(data)
##     if (!inherits(data, "matrix")) 
##         stop("data should be either a matrix or a data frame")
##     if ("data.frame" %in% class(MEDOIDS)) 
##         MEDOIDS = as.matrix(MEDOIDS)
##     if (is.null(MEDOIDS)) 
##         stop("the MEDOIDS should be a non-empty matrix or data frame")
##     if (ncol(MEDOIDS) != ncol(data)) 
##         stop("the MEDOIDS columns should be equal to the number of columns of the data")
##     if (!distance_metric %in% c("euclidean", "manhattan", "chebyshev", 
##         "canberra", "braycurtis", "pearson_correlation", "simple_matching_coefficient", 
##         "minkowski", "hamming", "jaccard_coefficient", "Rao_coefficient", 
##         "mahalanobis", "cosine")) 
##         stop("the distance_metric should be one of 'euclidean', 'manhattan', 'chebyshev', 'canberra', 'braycurtis', 'pearson_correlation', 'simple_matching_coefficient',\n         'minkowski', 'hamming', 'jaccard_coefficient', 'Rao_coefficient', 'mahalanobis', 'cosine'")
##     if (!is.logical(fuzzy)) 
##         stop("fuzzy should be either TRUE or FALSE")
##     if (distance_metric == "minkowski" && minkowski_p == 0) 
##         stop("if distance metric is minkowski then the minkowski_p should be either a positive or a negative number but not 0.0")
##     if (threads < 1) 
##         stop("threads should be an integer greater than 0")
##     flag_non_finite = check_NaN_Inf(data)
##     if (!flag_non_finite) 
##         stop("the data includes NaN's or +/- Inf values")
##     res = predict_medoids(data, distance_metric, MEDOIDS, minkowski_p, 
##         threads, fuzzy, 1e-06)
##     structure(list(call = match.call(), clusters = as.vector(res$clusters) + 
##         1, fuzzy_clusters = res$fuzzy_clusters, dissimilarity = res$dissimilarity, 
##         distance_metric = distance_metric), class = c("MedoidsCluster", 
##         "cluster medoids silhouette"))
## }
## <bytecode: 0x000001c58c3d17b8>
## <environment: namespace:ClusterR>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ClusterR}\SpecialCharTok{::}\NormalTok{Silhouette\_Dissimilarity\_Plot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (evaluation_object, silhouette = TRUE) 
## {
##     if (!"silhouette_plot" %in% names(evaluation_object)) {
##         if (!inherits(evaluation_object, "cluster medoids silhouette")) {
##             stop("the evaluation_object parameter should be the output of a Cluster_Medoids or Clara_Medoids function")
##         }
##     }
##     if (inherits(evaluation_object, "cluster medoids silhouette")) {
##         evaluation_object$silhouette_matrix = as.matrix(evaluation_object$silhouette_matrix)
##         evaluation_object = split_rcpp_lst(evaluation_object)
##     }
##     if (!is.logical(silhouette)) 
##         stop("silhouette should be either TRUE or FALSE")
##     if (grDevices::dev.cur() != 1) {
##         grDevices::dev.off()
##     }
##     success_plot_flag = rep(FALSE, length(evaluation_object$list_intra_dissm))
##     len_object = length(evaluation_object$list_intra_dissm)
##     op <- graphics::par(mfrow = c(len_object, 1), oma = c(2, 
##         2, 2.5, 2) + 0.1, mar = c(2, 2, 2, 2) + 0.1, mgp = c(2, 
##         1, 0))
##     max_ylim = max(unlist(lapply(evaluation_object$list_intra_dissm, 
##         length)))
##     for (i in 1:length(evaluation_object$list_intra_dissm)) {
##         if (silhouette) {
##             if (i == 1) {
##                 graphics::barplot(sort(as.vector(evaluation_object$list_silhouette[[i]]), 
##                   decreasing = F), width = 2, horiz = T, xlim = c(-1, 
##                   1), ylim = c(0, max_ylim), axes = F)
##                 if (len_object < 8) {
##                   graphics::legend("topleft", legend = c(paste0("cluster ", 
##                     i), paste0("silhouette : ", round(mean(evaluation_object$list_silhouette[[i]]), 
##                     3)), paste0("observations : ", length(evaluation_object$list_silhouette[[i]]))), 
##                     text.font = 2, cex = 1)
##                 }
##                 else {
##                   graphics::legend("topleft", legend = paste0("cluster ", 
##                     paste0(i, paste0(" , silhouette : ", paste0(round(mean(evaluation_object$list_silhouette[[i]]), 
##                       3)), paste0(" , observations : ", length(evaluation_object$list_silhouette[[i]]))))), 
##                     text.font = 2, cex = 1)
##                 }
##                 graphics::title(main = paste0("Silhouette plot for the ", 
##                   paste0(sum(unlist(lapply(evaluation_object$list_silhouette, 
##                     length))), " data observations")), cex.main = 1.5, 
##                   font.main = 4, col.main = "blue", outer = T)
##                 graphics::mtext(paste0("average silhouette width : ", 
##                   round(evaluation_object$avg_width_silhouette, 
##                     3)), outer = T, side = 1, cex = 0.75, font = 2, 
##                   col = "blue")
##             }
##             else if (i == length(evaluation_object$list_silhouette)) {
##                 graphics::barplot(sort(as.vector(evaluation_object$list_silhouette[[i]]), 
##                   decreasing = F), width = 2, horiz = T, xlim = c(-1, 
##                   1), ylim = c(0, max_ylim), axes = T)
##                 if (len_object < 8) {
##                   graphics::legend("topleft", legend = c(paste0("cluster ", 
##                     i), paste0("silhouette : ", round(mean(evaluation_object$list_silhouette[[i]]), 
##                     3)), paste0("observations : ", length(evaluation_object$list_silhouette[[i]]))), 
##                     text.font = 2, cex = 1)
##                 }
##                 else {
##                   graphics::legend("topleft", legend = paste0("cluster ", 
##                     paste0(i, paste0(" , silhouette : ", paste0(round(mean(evaluation_object$list_silhouette[[i]]), 
##                       3)), paste0(" , observations : ", length(evaluation_object$list_silhouette[[i]]))))), 
##                     text.font = 2, cex = 1)
##                 }
##             }
##             else {
##                 graphics::barplot(sort(as.vector(evaluation_object$list_silhouette[[i]]), 
##                   decreasing = F), width = 2, horiz = T, xlim = c(-1, 
##                   1), ylim = c(0, max_ylim), axes = F)
##                 if (len_object < 8) {
##                   graphics::legend("topleft", legend = c(paste0("cluster ", 
##                     i), paste0("silhouette : ", round(mean(evaluation_object$list_silhouette[[i]]), 
##                     3)), paste0("observations : ", length(evaluation_object$list_silhouette[[i]]))), 
##                     text.font = 2, cex = 1)
##                 }
##                 else {
##                   graphics::legend("topleft", legend = paste0("cluster ", 
##                     paste0(i, paste0(" , silhouette : ", paste0(round(mean(evaluation_object$list_silhouette[[i]]), 
##                       3)), paste0(" , observations : ", length(evaluation_object$list_silhouette[[i]]))))), 
##                     text.font = 2, cex = 1)
##                 }
##             }
##         }
##         if (!silhouette) {
##             max_dis = max(unlist(lapply(evaluation_object$list_intra_dissm, 
##                 mean)))
##             round_nearest_half = ceiling(max_dis/0.5) * 0.5
##             if (i == 1) {
##                 graphics::barplot(sort(as.vector(evaluation_object$list_intra_dissm[[i]]), 
##                   decreasing = F), width = 2, horiz = T, xlim = c(-0.5, 
##                   round_nearest_half + 0.1), ylim = c(0, max_ylim), 
##                   axes = F)
##                 if (len_object < 8) {
##                   graphics::legend("topleft", legend = c(paste0("cluster ", 
##                     i), paste0("dissimilarity : ", round(mean(evaluation_object$list_intra_dissm[[i]]), 
##                     3)), paste0("observations : ", length(evaluation_object$list_intra_dissm[[i]]))), 
##                     text.font = 2, cex = 1)
##                 }
##                 else {
##                   graphics::legend("topleft", legend = paste0("cluster ", 
##                     paste0(i, paste0(" , dissimilarity : ", paste0(round(mean(evaluation_object$list_intra_dissm[[i]]), 
##                       3)), paste0(" , observations : ", length(evaluation_object$list_intra_dissm[[i]]))))), 
##                     text.font = 2, cex = 1)
##                 }
##                 graphics::title(main = paste0("Dissimilarity plot for the ", 
##                   paste0(sum(unlist(lapply(evaluation_object$list_intra_dissm, 
##                     length))), " data observations")), cex.main = 1.5, 
##                   font.main = 4, col.main = "blue", outer = T)
##                 graphics::mtext(paste0("average dissimilarity : ", 
##                   round(evaluation_object$avg_intra_clust_dissimilarity, 
##                     3)), outer = T, side = 1, cex = 0.75, font = 2, 
##                   col = "blue")
##             }
##             else if (i == length(evaluation_object$list_intra_dissm)) {
##                 graphics::barplot(sort(as.vector(evaluation_object$list_intra_dissm[[i]]), 
##                   decreasing = F), width = 2, horiz = T, xlim = c(-0.5, 
##                   round_nearest_half + 0.1), ylim = c(0, max_ylim), 
##                   axes = T)
##                 if (len_object < 8) {
##                   graphics::legend("topleft", legend = c(paste0("cluster ", 
##                     i), paste0("dissimilarity : ", round(mean(evaluation_object$list_intra_dissm[[i]]), 
##                     3)), paste0("observations : ", length(evaluation_object$list_intra_dissm[[i]]))), 
##                     text.font = 2, cex = 1)
##                 }
##                 else {
##                   graphics::legend("topleft", legend = paste0("cluster ", 
##                     paste0(i, paste0(" , dissimilarity : ", paste0(round(mean(evaluation_object$list_intra_dissm[[i]]), 
##                       3)), paste0(" , observations : ", length(evaluation_object$list_intra_dissm[[i]]))))), 
##                     text.font = 2, cex = 1)
##                 }
##             }
##             else {
##                 graphics::barplot(sort(as.vector(evaluation_object$list_intra_dissm[[i]]), 
##                   decreasing = F), width = 2, horiz = T, xlim = c(-0.5, 
##                   round_nearest_half + 0.1), ylim = c(0, max_ylim), 
##                   axes = F)
##                 if (len_object < 8) {
##                   graphics::legend("topleft", legend = c(paste0("cluster ", 
##                     i), paste0("dissimilarity : ", round(mean(evaluation_object$list_intra_dissm[[i]]), 
##                     3)), paste0("observations : ", length(evaluation_object$list_intra_dissm[[i]]))), 
##                     text.font = 2, cex = 1)
##                 }
##                 else {
##                   graphics::legend("topleft", legend = paste0("cluster ", 
##                     paste0(i, paste0(" , dissimilarity : ", paste0(round(mean(evaluation_object$list_intra_dissm[[i]]), 
##                       3)), paste0(" , observations : ", length(evaluation_object$list_intra_dissm[[i]]))))), 
##                     text.font = 2, cex = 1)
##                 }
##             }
##         }
##         success_plot_flag[i] = TRUE
##     }
##     if (sum(success_plot_flag) == length(evaluation_object$list_intra_dissm)) {
##         return(T)
##     }
##     else {
##         return(F)
##     }
## }
## <bytecode: 0x000001c58c46d468>
## <environment: namespace:ClusterR>
\end{verbatim}

ClusterR only supports Distribution-based and Centroid-based algorithms.

The only distribution-based algorithm in ClusterR is GMM (Gaussian
Mixture Model). GMM models the data-generating process as a set of
gaussian models, one for each cluster. Doing so, each gaussian (cluster)
gives the probability density function of the points it generates. A
point belongs to the cluster that maximized the probability of
generating it there.

GMM uses generalized gaussian distributions, so each cluster has its own
independent mean (centroid) and variance (diagonal covariance matrix).

For the Centroid-based algorithms, ClusterR supports K-means and
K-medoids variations.

K-means initially creates k random points which represents the
centroids. Then, for each iteration, the algorithm moves the centroid
positions to optimize the clustering. The k-medoid algorithms are a
variation of k-means, which add the constraint that the centroids must
be placed over the item positions.

For testing the GMM algorithm, we have created a synthetic dataset drawn
from three 2-dimensional gaussians.

To do it we used the library MASS, which defines the function mvrnorm:
specifying the mu (mean) and sigma (correlation matrix), along with the
desidered dimensionality and number of samples, it returns a matrix
representing n R\^{}k points. Then we combined three of these matrix
(one for each gaussian generating process) into a single dataframe,
where the column ``color'' specifies the class for each point.

We finally visualized the points using a 2-d colored scatterplot; the
digits indicate the positions of the centroids.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load library MASS}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: il pacchetto 'dplyr' Ã¨ stato creato con R versione 4.2.2
\end{verbatim}

\begin{verbatim}
## 
## Caricamento pacchetto: 'dplyr'
\end{verbatim}

\begin{verbatim}
## Il seguente oggetto Ã¨ mascherato da 'package:MASS':
## 
##     select
\end{verbatim}

\begin{verbatim}
## I seguenti oggetti sono mascherati da 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## I seguenti oggetti sono mascherati da 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ClusterR)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: il pacchetto 'ClusterR' Ã¨ stato creato con R versione 4.2.2
\end{verbatim}

\begin{verbatim}
## Caricamento del pacchetto richiesto: gtools
\end{verbatim}

\begin{verbatim}
## Warning: il pacchetto 'gtools' Ã¨ stato creato con R versione 4.2.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load\_factor }\OtherTok{=} \DecValTok{100}
\NormalTok{ncol}\OtherTok{=}\DecValTok{2}

\NormalTok{coeff }\OtherTok{=} \FunctionTok{log}\NormalTok{(}\AttributeTok{base=}\DecValTok{10}\NormalTok{, load\_factor)}\SpecialCharTok{/}\DecValTok{2}    \CommentTok{\#choose if augmenting distance when generating more points}
\CommentTok{\#coeff = 1}
  
\CommentTok{\# create bivariate normal distribution}
\NormalTok{df1 }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10} \SpecialCharTok{*}\NormalTok{ load\_factor, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff, }\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{Sigma =} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{6}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)), }\AttributeTok{color=}\DecValTok{1}\NormalTok{),}
  \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10} \SpecialCharTok{*}\NormalTok{ load\_factor, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff, }\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{Sigma =} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(.}\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)), }\AttributeTok{color=}\DecValTok{2}\NormalTok{),}
  \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{15} \SpecialCharTok{*}\NormalTok{ load\_factor, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{coeff, }\DecValTok{1}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{Sigma =} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, .}\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)), }\AttributeTok{color=}\DecValTok{3}\NormalTok{)}
\NormalTok{))}

\FunctionTok{plot}\NormalTok{(}\AttributeTok{x=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X1, }\AttributeTok{y=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X2, }\AttributeTok{col=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{color)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"1"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"brown"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"2"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"brown"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"3"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"brown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-2-1.pdf}

The function given by the library to compute Gaussian Mixture Models
(GMM) is GMM. It takes two compulsory arguments: the matrix specifying
the item positions (one row for each item, one column for each
component) and the number of gaussian processes.

The other parameters are: - dist\_mode: specifies if the training
algorithm should use euclidean or manhattan distance - seed mode:
specifies the initial placement of the centroids for the iterative
algorithm (static/random subset/spread) - number of iterations for the
k-means algorithm - number of iterations for the
expectation-maximization algorithm - smallest possible values for
diagonal covariances - seed for random number generator (to make the
experiment replicable) - whether or not to plot the results in a
graphical form - verbosity

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{subset}\NormalTok{(df1, }\AttributeTok{select=}\FunctionTok{c}\NormalTok{(X1, X2))}
\NormalTok{gmm\_3 }\OtherTok{=} \FunctionTok{GMM}\NormalTok{(x, }\DecValTok{3}\NormalTok{, }\AttributeTok{dist\_mode =} \StringTok{"maha\_dist"}\NormalTok{, }\AttributeTok{seed\_mode =} \StringTok{"random\_subset"}\NormalTok{)          }

\CommentTok{\# predict centroids, covariance matrix and weights}
\NormalTok{df1[}\StringTok{"color\_pred\_3"}\NormalTok{] }\OtherTok{=} \FunctionTok{predict}\NormalTok{(gmm\_3, }\AttributeTok{newdata =}\NormalTok{ x)}

\FunctionTok{plot}\NormalTok{(}\AttributeTok{x=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X1, }\AttributeTok{y=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X2, }\AttributeTok{col=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{color\_pred\_3 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"1"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"2"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"3"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(gmm\_3}\SpecialCharTok{$}\NormalTok{centroids, }\AttributeTok{pch=}\StringTok{"X"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"azure"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-3-1.pdf}

It is also possible to use the \emph{plot\_2d} function, defined by by
the ClusterR package, for a 2-d visualization of the predicted points
along with the centroid/medoid positions.

The arguments are: - data, 2-dimensional matrix specifying the item
positions - clusters, a list specifying the class for each item (numeric
vector) - centroids\_medoids, the position of the centroids (or medoids)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_2d}\NormalTok{(}\AttributeTok{data =}\NormalTok{ x, }\AttributeTok{clusters =}\NormalTok{ df1}\SpecialCharTok{$}\NormalTok{color\_pred\_3, }\AttributeTok{centroids\_medoids =}\NormalTok{ gmm\_3}\SpecialCharTok{$}\NormalTok{centroids)}
\end{Highlighting}
\end{Shaded}

The function \emph{external\_validation}, also defined in the ClusterR
package, gives us the possibility to extract some useful metrics
measuring the goodness of the fit. It requires only the predicted values
and annotated values.

The arguments are: - true\_labels: the annotated values - clusters: the
predicted values - method for calculating the goodness (the value
returned by the function. although the function shows all the measures
in the log) - summary\_stats: whether or not to print summary statistics
like sensitivity and specificity

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{external\_validation}\NormalTok{(df1}\SpecialCharTok{$}\NormalTok{color, df1}\SpecialCharTok{$}\NormalTok{color\_pred\_3, }\AttributeTok{summary\_stats =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  
## ---------------------------------------- 
## purity                         : 0.864 
## entropy                        : 0.3474 
## normalized mutual information  : 0.6568 
## variation of information       : 1.0767 
## normalized var. of information : 0.511 
## ---------------------------------------- 
## specificity                    : 0.8834 
## sensitivity                    : 0.7463 
## precision                      : 0.7725 
## recall                         : 0.7463 
## F-measure                      : 0.7592 
## ---------------------------------------- 
## accuracy OR rand-index         : 0.8358 
## adjusted-rand-index            : 0.6347 
## jaccard-index                  : 0.6118 
## fowlkes-mallows-index          : 0.7593 
## mirkin-metric                  : 2010598 
## ----------------------------------------
\end{verbatim}

\begin{verbatim}
## [1] 0.6346912
\end{verbatim}

The GMM object has 4 attributes: - centroids: a matrix that specifies
the position of one centroid per row - covariance\_matrices: a matrix
that specifies one diagonal covariance matrix per row - weights, for
each gaussian component (list) - Log\_likelihood: a matrix one row for
each training item and one column for each component

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gmm\_3}\SpecialCharTok{$}\NormalTok{centroids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]
## [1,] 3.3949062 0.9109905
## [2,] 0.2030739 0.1031370
## [3,] 4.9087278 4.8275408
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gmm\_3}\SpecialCharTok{$}\NormalTok{covariance\_matrices}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]     [,2]
## [1,] 2.209386 1.416455
## [2,] 1.002823 3.365906
## [3,] 1.085786 1.260057
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gmm\_3}\SpecialCharTok{$}\NormalTok{weights}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3225477 0.3598333 0.3176190
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(gmm\_3}\SpecialCharTok{$}\NormalTok{Log\_likelihood)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]      [,2]      [,3]
## [1,] -6.220771 -26.26571 -6.147051
## [2,] -7.093511 -12.21441 -2.487667
## [3,] -8.950454 -17.08605 -2.013067
## [4,] -6.654050 -15.86106 -2.168784
## [5,] -7.560245 -21.19396 -2.650248
## [6,] -9.593289 -18.59807 -2.079115
\end{verbatim}

Since it is possible to specify the number of clusters, we tried running
the GMM algorithm specifying gaussian\_comps=2.

We can see that the clustering still makes sense, even if the external
validation statistics are worse. One of the centroids remains on the
cluster nr.1, while the other centroid is in the middle of clusters 2
and 3.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{subset}\NormalTok{(df1, }\AttributeTok{select=}\FunctionTok{c}\NormalTok{(X1, X2))}
\NormalTok{gmm\_2 }\OtherTok{=} \FunctionTok{GMM}\NormalTok{(x, }\DecValTok{2}\NormalTok{, }\AttributeTok{dist\_mode =} \StringTok{"maha\_dist"}\NormalTok{, }\AttributeTok{seed\_mode =} \StringTok{"random\_subset"}\NormalTok{)          }

\CommentTok{\# predict centroids, covariance matrix and weights}
\NormalTok{df1[}\StringTok{"color\_pred\_2"}\NormalTok{] }\OtherTok{=} \FunctionTok{predict}\NormalTok{(gmm\_2, }\AttributeTok{newdata =}\NormalTok{ x)}

\FunctionTok{plot}\NormalTok{(}\AttributeTok{x=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X1, }\AttributeTok{y=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X2, }\AttributeTok{col=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{color\_pred\_2 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"1"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"2"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"3"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(gmm\_2}\SpecialCharTok{$}\NormalTok{centroids, }\AttributeTok{pch=}\StringTok{"X"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"azure"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{external\_validation}\NormalTok{(df1}\SpecialCharTok{$}\NormalTok{color, df1}\SpecialCharTok{$}\NormalTok{color\_pred\_2, }\AttributeTok{summary\_stats =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  
## ---------------------------------------- 
## purity                         : 0.576 
## entropy                        : 0.2472 
## normalized mutual information  : 0.4656 
## variation of information       : 1.3572 
## normalized var. of information : 0.6966 
## ---------------------------------------- 
## specificity                    : 0.6246 
## sensitivity                    : 0.7682 
## precision                      : 0.5207 
## recall                         : 0.7682 
## F-measure                      : 0.6207 
## ---------------------------------------- 
## accuracy OR rand-index         : 0.6744 
## adjusted-rand-index            : 0.3534 
## jaccard-index                  : 0.45 
## fowlkes-mallows-index          : 0.6324 
## mirkin-metric                  : 3987488 
## ----------------------------------------
\end{verbatim}

\begin{verbatim}
## [1] 0.3533881
\end{verbatim}

How to find the optimal number of centroids? Doing so by plotting the
data with colors only works for small datasets representable in two
dimensions. The function Optimal\_Clusters\_GMM calculates the goodness
of the fit for each \# of clusters between 1 and max\_clusters.

The criterion can be AIC (Alkaine Information Criterion) or BIC (Bayes
Information Criterion). A rule of thumb is that we should select the
smallest (simplest) model with good performances, so the best model
should be the one which minimized one of these criterions.

The other parameters are: - dist\_mode: specifies if the training
algorithm should use euclidean or manhattan distance - seed mode:
specifies the initial placement of the centroids for the iterative
algorithm (static/random subset/spread) - number of iterations for the
k-means algorithm - number of iterations for the
expectation-maximization algorithm - smallest possible values for
diagonal covariances - seed for random number generator (to make the
experiment replicable) - whether or not to plot the results in a
graphical form - verbosity

In the output we can see that a model with 3 clusters should be fine,
because after this value the BIC stops increasing sharply. Models with a
lot of clusters have a even lower value for BIC but may be too complex.
This shows why it is important to watch also the clustering results
using a scatterplot, if possible.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{subset}\NormalTok{(df1, }\AttributeTok{select=}\FunctionTok{c}\NormalTok{(X1, X2))}
\NormalTok{opt\_gmm }\OtherTok{=} \FunctionTok{Optimal\_Clusters\_GMM}\NormalTok{(x, }\AttributeTok{max\_clusters =} \DecValTok{10}\NormalTok{, }\AttributeTok{criterion =} \StringTok{"BIC"}\NormalTok{, }
                               \AttributeTok{dist\_mode =} \StringTok{"eucl\_dist"}\NormalTok{, }\AttributeTok{seed\_mode =} \StringTok{"random\_subset"}\NormalTok{,}
                               \AttributeTok{plot\_data =}\NormalTok{ T)      }
\end{Highlighting}
\end{Shaded}

ClusterR supports k-means algorithm with two different implementations:
KMeans\_arma and KMeans\_rcpp. The difference is that KMeans\_rcpp: -
allows for the specification of the initial centroid positions and other
initialization methods - the running time and convergence can be set by
the user - the num\_init parameter KMeans\_arma is faster than
KMeans\_rcpp.

Their interface is similar to GMM, but specific for k-means task. The
arguments for KMeans\_rcpp are: - data: a matrix specifying the item
positions (items on rows, components on columns) - clusters: the number
of clusters - num\_init: how many times to run the algorithm with
different initializations. If \textgreater1, the function returns the
best clustering by within-cluster-sum-of-squared-error - max\_iters: max
number of iterations - initializer: how to initialize the centroids
(optimal\_init, quantile\_init, kmeans++, random) - fuzzy: if true, the
prediction probabilities are proportional to the distances to the
centroids - CENTROIDS: a matrix specifying the initial positions (one
row per centroid, one column per component) - tol: tolerance for
convergence - tol\_optimal\_init: tolerance value when using the
``optimal\_init'' initializer (higher means further inital positions) -
seed of the RNG (for reproducibility) - verbosity

In the following the result of k-means clustering with k=3 and
KMeans\_rcpp implementation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(KMeans\_arma)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## avvio in corso del server httpd per la guida ... fatto
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{subset}\NormalTok{(df1, }\AttributeTok{select=}\FunctionTok{c}\NormalTok{(X1, X2))}
\NormalTok{kmeans\_rcpp }\OtherTok{=} \FunctionTok{KMeans\_rcpp}\NormalTok{(x, }\DecValTok{3}\NormalTok{)          }

\CommentTok{\# predict centroids, covariance matrix and weights}
\NormalTok{df1[}\StringTok{"color\_pred\_kmeans\_3"}\NormalTok{] }\OtherTok{=} \FunctionTok{predict}\NormalTok{(kmeans\_rcpp, }\AttributeTok{newdata =}\NormalTok{ x)}

\FunctionTok{plot}\NormalTok{(}\AttributeTok{x=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X1, }\AttributeTok{y=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X2, }\AttributeTok{col=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{color\_pred\_kmeans\_3 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"1"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"2"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"3"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(kmeans\_rcpp}\SpecialCharTok{$}\NormalTok{centroids, }\AttributeTok{pch=}\StringTok{"X"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"azure"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-9-1.pdf}

\ldots{} todo comparison of running time between rcpp and arma\ldots{}

The k-medoid variations are supported by the implementations
Cluster\_Medoids and Clara\_Medoids.

\ldots{} todo cluster/clara documentation\ldots{} \ldots{} todo cluster
vs kmeans for very small datasets (cluster should place centroids on
items)\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{=} \FunctionTok{subset}\NormalTok{(df1, }\AttributeTok{select=}\FunctionTok{c}\NormalTok{(X1, X2))}
\NormalTok{cluster\_med }\OtherTok{=} \FunctionTok{Cluster\_Medoids}\NormalTok{(x, }\DecValTok{3}\NormalTok{)          }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: The `seed` argument of `Cluster_Medoids()` is deprecated as of ClusterR 1.2.6.
## i The 'seed' parameter will be removed in version 1.3.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# predict centroids, covariance matrix and weights}
\NormalTok{df1[}\StringTok{"color\_pred\_cluster\_3"}\NormalTok{] }\OtherTok{=} \FunctionTok{predict}\NormalTok{(cluster\_med, }\AttributeTok{newdata =}\NormalTok{ x)}

\FunctionTok{plot}\NormalTok{(}\AttributeTok{x=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X1, }\AttributeTok{y=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{X2, }\AttributeTok{col=}\NormalTok{df1}\SpecialCharTok{$}\NormalTok{color\_pred\_cluster\_3 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"1"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"2"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\SpecialCharTok{*}\NormalTok{coeff), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{*}\NormalTok{coeff), }\AttributeTok{pch=}\StringTok{"3"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(cluster\_med}\SpecialCharTok{$}\NormalTok{medoids, }\AttributeTok{pch=}\StringTok{"X"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\StringTok{"azure"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-10-1.pdf}

\ldots{} todo batch kmeans (doc, running time for large
datasets)\ldots{}

\ldots todo silouhette plot for kmeans\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(Silhouette\_Dissimilarity\_Plot)}

\FunctionTok{data}\NormalTok{(soybean)}

\NormalTok{dat }\OtherTok{=}\NormalTok{ soybean[, }\SpecialCharTok{{-}}\FunctionTok{ncol}\NormalTok{(soybean)]}

\NormalTok{cm }\OtherTok{=} \FunctionTok{Cluster\_Medoids}\NormalTok{(dat, }\AttributeTok{clusters =} \DecValTok{5}\NormalTok{, }\AttributeTok{distance\_metric =} \StringTok{\textquotesingle{}jaccard\_coefficient\textquotesingle{}}\NormalTok{)}

\NormalTok{plt\_sd }\OtherTok{=} \FunctionTok{Silhouette\_Dissimilarity\_Plot}\NormalTok{(cm, }\AttributeTok{silhouette =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the package ClusterR there are 3 different datasets: -
dietary\_survey\_IBS: Synthetic data using a dietary survey of patients
with irritable bowel syndrome. - mushroom: The mushroom data - soybean:
The soybean (large) data set from the UCI repository

\emph{dietary\_survey\_IBS} contains synthetic data generated using the
mean and standard deviation of data used in the paper ``A dietary survey
of patients with irritable bowel syndrome''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(dietary\_survey\_IBS)}
\end{Highlighting}
\end{Shaded}

It contains 400 rows and 43 columns.

``class'' is the target column. Its values are O and 1, with 0 meaning
healty and 1 meaning IBS-positive.

The other 42 columns are numeric and can be used as predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(dietary\_survey\_IBS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 400  43
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(dietary\_survey\_IBS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "bread"                      "wheat"                     
##  [3] "pasta"                      "breakfast_cereal"          
##  [5] "yeast"                      "spicy_food"                
##  [7] "curry"                      "chinese_takeaway"          
##  [9] "chilli"                     "cabbage"                   
## [11] "onion"                      "garlic"                    
## [13] "potatoes"                   "pepper"                    
## [15] "vegetables_unspecified"     "tomato"                    
## [17] "beans_and_pulses"           "mushroom"                  
## [19] "fatty_foods_unspecified"    "sauces"                    
## [21] "chocolate"                  "fries"                     
## [23] "crisps"                     "desserts"                  
## [25] "eggs"                       "red_meat"                  
## [27] "processed_meat"             "pork"                      
## [29] "chicken"                    "fish_shellfish"            
## [31] "dairy_products_unspecified" "cheese"                    
## [33] "cream"                      "milk"                      
## [35] "fruit_unspecified"          "nuts_and_seeds"            
## [37] "orange"                     "apple"                     
## [39] "banana"                     "grapes"                    
## [41] "alcohol"                    "caffeine"                  
## [43] "class"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(dietary\_survey\_IBS}\SpecialCharTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(dietary\_survey\_IBS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       bread     wheat     pasta breakfast_cereal      yeast spicy_food
## 1  22.20456  26.65530 15.989490         5.849367  0.9299678  66.241185
## 2  52.54733  46.27706 21.131506        14.636202 -1.1443742  65.649463
## 3  14.36977  44.70844  5.486241         9.993163  6.7732122  14.713838
## 4 105.42122  15.30204  6.814486        10.059338  3.9468686  42.497711
## 5  58.01079 -14.66920  6.655731         6.823591  2.2974212  38.866803
## 6  14.93761  58.68050  6.909726         2.056893  3.4807815  -5.629262
##       curry chinese_takeaway   chilli  cabbage     onion    garlic  potatoes
## 1 -2.735761        20.724598 6.484320 19.21056  5.049086 14.951061  6.122586
## 2 36.041828        11.056420 5.356049 18.01014 -3.496564 14.220131 14.394248
## 3 -6.076168         7.794958 2.339704 11.42357 25.716985  3.894881  9.000055
## 4 -8.853831         4.368930 6.479576 26.38321 16.640787 -9.533368 17.338856
## 5 22.386713         5.718093 4.708961 10.22217 11.897280  4.876392 11.765393
## 6 24.724332         3.326057 8.449127 31.09404 18.101876 17.546202  9.243476
##      pepper vegetables_unspecified     tomato beans_and_pulses  mushroom
## 1 11.463460             -2.6877943  7.0546731         4.235265 0.8397533
## 2 10.677281              5.3887746  5.8635421         7.604121 2.7188960
## 3  7.063448              8.7510534  8.9377498         3.983618 3.2943628
## 4  6.784303             -0.5949138  1.8419248         9.589967 7.5955588
## 5 14.621550              8.5547661 -0.8581522        11.861209 5.6351146
## 6 -5.784106              1.5434930  7.7106658         2.656740 3.7338154
##   fatty_foods_unspecified    sauces   chocolate      fries   crisps   desserts
## 1               41.500297  6.909797  1.63302477 -0.5521378 2.572284  2.3864425
## 2                9.373348 12.421257 16.05186013 12.1469611 8.352107 -0.5918975
## 3               -1.679362  8.220496 14.58015302 -4.3049715 4.704147  5.1747127
## 4               38.493166 16.435779 -0.01494668 -1.3557655 8.770372  2.5694979
## 5               28.550844  6.079985 -5.16385024  8.7490976 2.712111  5.0838668
## 6                7.508147 30.400592 14.73023773  7.0734452 4.169258  8.9053153
##        eggs   red_meat processed_meat      pork  chicken fish_shellfish
## 1  1.908757 -1.4530299      10.882347  9.157404 5.845275      1.8634543
## 2 19.544320 13.5456842      18.396438 12.230256 3.174840      1.5988309
## 3  6.992720 -2.5842750      16.280947 -3.956996 5.048465      2.7946304
## 4 23.111712 17.3950598       9.099817  3.429923 5.440719      2.5085347
## 5  8.101268 16.9607494       3.560615  7.022158 1.217705      1.3953498
## 6 11.763982  0.4948489      13.514120 13.795242 4.944823      0.9535027
##   dairy_products_unspecified    cheese      cream       milk fruit_unspecified
## 1                   9.041778  1.677664  9.4404424  4.1592333          8.483641
## 2                   3.631816 10.162022  6.2131946  9.7200990         10.770151
## 3                  -8.067248  5.990827 11.9530082  1.1129422          3.004165
## 4                  15.520612 11.366135  0.2715079 -0.3178373          6.176477
## 5                  12.994296  9.953474 -0.4398934 11.3568021          3.825484
## 6                  21.759154 18.642572  7.6811231  1.0137448         -1.105803
##   nuts_and_seeds   orange      apple   banana   grapes   alcohol  caffeine
## 1    -0.01067082 6.328142  4.3583062 5.718117 4.214223 23.359110 19.061011
## 2    10.58545050 7.969777 -0.1491200 2.994223 2.184413  6.918144  5.796814
## 3    10.15751386 5.298184  3.1657997 4.861562 1.403917 -2.589836 51.885415
## 4    11.09171706 9.704125 -0.4869209 1.155338 3.116459 44.720107 20.523801
## 5     4.52496999 5.891424  4.7103904 3.107612 1.642572 -3.833479 40.963779
## 6     5.54772626 7.790069  5.9195136 3.812729 3.424312 16.522105 37.420165
##   class
## 1     1
## 2     1
## 3     1
## 4     1
## 5     1
## 6     1
\end{verbatim}

In the following a 2d PCA for the entire dataset: black for class 0
(healty) and red for class 1 (positive).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_dat }\OtherTok{=}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{princomp}\NormalTok{(dietary\_survey\_IBS)}\SpecialCharTok{$}\NormalTok{scores[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\FunctionTok{plot}\NormalTok{(pca\_dat, }\AttributeTok{col=}\NormalTok{dietary\_survey\_IBS}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-14-1.pdf}

\emph{mushroom} includes descriptions of hypothetical samples
corresponding to 23 species of gilled mushrooms in the Agaricus and
Lepiota Family (pp.~500-525).

Each species can either be edible or poisonous. The Guide clearly states
that there is no simple rule for determining the edibility of a
mushroom; no rule like `leaflets three, let it be' for Poisonous Oak and
Ivy. For simplicity the unknown targets are marked as poisounous.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(mushroom)}
\end{Highlighting}
\end{Shaded}

It contains 8124 rows and 23 columns.

``class'' is the target column. Its values are ``p'' and ``e'', with
``p'' meaning poisounous (or unknown) and e meaning ``edible''.

The other 22 columns contain one character each and can be used as
predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(mushroom)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8124   23
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(mushroom)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "class"                    "cap_shape"               
##  [3] "cap_surface"              "cap_color"               
##  [5] "bruises"                  "odor"                    
##  [7] "gill_attachment"          "gill_spacing"            
##  [9] "gill_size"                "gill_color"              
## [11] "stalk_shape"              "stalk_root"              
## [13] "stalk_surface_above_ring" "stalk_surface_below_ring"
## [15] "stalk_color_above_ring"   "stalk_color_below_ring"  
## [17] "veil_type"                "veil_color"              
## [19] "ring_number"              "ring_type"               
## [21] "spore_print_color"        "population"              
## [23] "habitat"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(mushroom}\SpecialCharTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] p e
## Levels: e p
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(mushroom)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   class cap_shape cap_surface cap_color bruises odor gill_attachment
## 1     p         x           s         n       t    p               f
## 2     e         x           s         y       t    a               f
## 3     e         b           s         w       t    l               f
## 4     p         x           y         w       t    p               f
## 5     e         x           s         g       f    n               f
## 6     e         x           y         y       t    a               f
##   gill_spacing gill_size gill_color stalk_shape stalk_root
## 1            c         n          k           e          e
## 2            c         b          k           e          c
## 3            c         b          n           e          c
## 4            c         n          n           e          e
## 5            w         b          k           t          e
## 6            c         b          n           e          c
##   stalk_surface_above_ring stalk_surface_below_ring stalk_color_above_ring
## 1                        s                        s                      w
## 2                        s                        s                      w
## 3                        s                        s                      w
## 4                        s                        s                      w
## 5                        s                        s                      w
## 6                        s                        s                      w
##   stalk_color_below_ring veil_type veil_color ring_number ring_type
## 1                      w         p          w           o         p
## 2                      w         p          w           o         p
## 3                      w         p          w           o         p
## 4                      w         p          w           o         p
## 5                      w         p          w           o         e
## 6                      w         p          w           o         p
##   spore_print_color population habitat
## 1                 k          s       u
## 2                 n          n       g
## 3                 n          n       m
## 4                 k          s       u
## 5                 n          a       g
## 6                 k          n       g
\end{verbatim}

In the following a 2d PCA for the entire dataset: black for class 0
(edible) and red for class 1 (poisounous).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{=}\NormalTok{ mushroom[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{y }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(mushroom[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{])            }

\NormalTok{gwd }\OtherTok{=}\NormalTok{ FD}\SpecialCharTok{::}\FunctionTok{gowdis}\NormalTok{(X)           }

\NormalTok{gwd\_mat }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(gwd)                 }

\NormalTok{pca\_dat }\OtherTok{=}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{princomp}\NormalTok{(gwd\_mat)}\SpecialCharTok{$}\NormalTok{scores[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\FunctionTok{plot}\NormalTok{(pca\_dat, }\AttributeTok{col=}\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-17-1.pdf}

\emph{soybean} contains a dataset about soybeans characteristics from
UCI machine learning repository.

There are both categorical and ordinal values, encoded by integers.
Missing values were imputated using the mice package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{help}\NormalTok{(soybean)}
\end{Highlighting}
\end{Shaded}

It contains 307 rows and 36 columns.

``class'' is the target column. Its values are
``diaporthe-stem-canker'', ``charcoal-rot'', ``rhizoctonia-root-rot'',
``phytophthora-rot'', ``brown-stem-rot'', ``powdery-mildew'',
``downy-mildew'', ``brown-spot'', ``bacterial-blight'',
``bacterial-pustule'', ``purple-seed-stain'', ``anthracnose'',
``phyllosticta-leaf-spot'', ``alternarialeaf-spot'',
``frog-eye-leaf-spot'', ``diaporthe-pod-\&-stem-blight'',
``cyst-nematode'', ``2-4-d-injury'' and ``herbicide-injury''.

The other 25 columns contain one integer each and can be used as
predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(soybean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 307  36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(soybean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "date"            "plant_stand"     "precip"          "temp"           
##  [5] "hail"            "crop_hist"       "area_damaged"    "severity"       
##  [9] "seed_tmt"        "germination"     "plant_growth"    "leaves"         
## [13] "leafspots_halo"  "leafspots_marg"  "leafspot_size"   "leaf_shread"    
## [17] "leaf_malf"       "leaf_mild"       "stem"            "lodging"        
## [21] "stem_cankers"    "canker_lesion"   "fruiting_bodies" "external_decay" 
## [25] "mycelium"        "int_discolor"    "sclerotia"       "fruit_pods"     
## [29] "fruit_spots"     "seed"            "mold_growth"     "seed_discolor"  
## [33] "seed_size"       "shriveling"      "roots"           "class"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(soybean}\SpecialCharTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] diaporthe-stem-canker       charcoal-rot               
##  [3] rhizoctonia-root-rot        phytophthora-rot           
##  [5] brown-stem-rot              powdery-mildew             
##  [7] downy-mildew                brown-spot                 
##  [9] bacterial-blight            bacterial-pustule          
## [11] purple-seed-stain           anthracnose                
## [13] phyllosticta-leaf-spot      alternarialeaf-spot        
## [15] frog-eye-leaf-spot          diaporthe-pod-&-stem-blight
## [17] cyst-nematode               2-4-d-injury               
## [19] herbicide-injury           
## 19 Levels: 2-4-d-injury alternarialeaf-spot anthracnose ... rhizoctonia-root-rot
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(soybean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   date plant_stand precip temp hail crop_hist area_damaged severity seed_tmt
## 1    6           0      2    1    0         1            1        1        0
## 2    4           0      2    1    0         2            0        2        1
## 3    3           0      2    1    0         1            0        2        1
## 4    3           0      2    1    0         1            0        2        0
## 5    6           0      2    1    0         2            0        1        0
## 6    5           0      2    1    0         3            0        1        0
##   germination plant_growth leaves leafspots_halo leafspots_marg leafspot_size
## 1           0            1      1              0              2             2
## 2           1            1      1              0              2             2
## 3           2            1      1              0              2             2
## 4           1            1      1              0              2             2
## 5           2            1      1              0              2             2
## 6           1            1      1              0              2             2
##   leaf_shread leaf_malf leaf_mild stem lodging stem_cankers canker_lesion
## 1           0         0         0    1       1            3             1
## 2           0         0         0    1       0            3             1
## 3           0         0         0    1       0            3             0
## 4           0         0         0    1       0            3             0
## 5           0         0         0    1       0            3             1
## 6           0         0         0    1       0            3             0
##   fruiting_bodies external_decay mycelium int_discolor sclerotia fruit_pods
## 1               1              1        0            0         0          0
## 2               1              1        0            0         0          0
## 3               1              1        0            0         0          0
## 4               1              1        0            0         0          0
## 5               1              1        0            0         0          0
## 6               1              1        0            0         0          0
##   fruit_spots seed mold_growth seed_discolor seed_size shriveling roots
## 1           4    0           0             0         0          0     0
## 2           4    0           0             0         0          0     0
## 3           4    0           0             0         0          0     0
## 4           4    0           0             0         0          0     0
## 5           4    0           0             0         0          0     0
## 6           4    0           0             0         0          0     0
##                   class
## 1 diaporthe-stem-canker
## 2 diaporthe-stem-canker
## 3 diaporthe-stem-canker
## 4 diaporthe-stem-canker
## 5 diaporthe-stem-canker
## 6 diaporthe-stem-canker
\end{verbatim}

In the following a 2d PCA for the entire dataset: one color for each
different class.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{=}\NormalTok{ soybean[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{y }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{(soybean[, }\DecValTok{1}\NormalTok{])            }

\NormalTok{gwd }\OtherTok{=}\NormalTok{ FD}\SpecialCharTok{::}\FunctionTok{gowdis}\NormalTok{(X)           }

\NormalTok{gwd\_mat }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(gwd)                 }

\NormalTok{pca\_dat }\OtherTok{=}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{princomp}\NormalTok{(gwd\_mat)}\SpecialCharTok{$}\NormalTok{scores[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\FunctionTok{plot}\NormalTok{(pca\_dat, }\AttributeTok{col=}\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\includegraphics{main_files/figure-latex/unnamed-chunk-20-1.pdf}

\ldots todo test clustering on datasets\ldots{}

\end{document}
